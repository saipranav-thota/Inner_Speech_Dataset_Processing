{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split EEG Signals into Individual Channels\n",
    "\n",
    "This notebook takes the processed EEG epochs and splits them into individual channel files.\n",
    "\n",
    "**Input**: `processed_epochs/` folder with processed MNE epoch files  \n",
    "**Output**: `individual_channels/` folder with separate files for each channel\n",
    "\n",
    "## Output Structure:\n",
    "```\n",
    "individual_channels/\n",
    "├── by_epoch/\n",
    "│   ├── sub-01_ses-01_trial_000_Inner_Up/\n",
    "│   │   ├── A1.npy\n",
    "│   │   ├── A2.npy\n",
    "│   │   └── ... (128 channel files)\n",
    "│   └── ...\n",
    "├── by_channel/\n",
    "│   ├── A1/\n",
    "│   │   ├── sub-01_ses-01_trial_000_Inner_Up.npy\n",
    "│   │   └── ...\n",
    "│   └── ...\n",
    "└── metadata/\n",
    "    ├── channel_files_metadata.csv\n",
    "    └── split_summary.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "# Configure settings\n",
    "mne.set_log_level('WARNING')\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: D:\\VIT\\IV-Year\\PJT-I\\Speech Imagery Decoding\\Inner_Speech_Dataset\\Dataset\\processed_epochs\n",
      "Output: D:\\VIT\\IV-Year\\PJT-I\\Speech Imagery Decoding\\Inner_Speech_Dataset\\Dataset\\individual_channels\n",
      "\n",
      "Output structure:\n",
      "  D:\\VIT\\IV-Year\\PJT-I\\Speech Imagery Decoding\\Inner_Speech_Dataset\\Dataset\\individual_channels/\n",
      "    ├── by_epoch/     (folders per epoch, files per channel)\n",
      "    ├── by_channel/   (folders per channel, files per epoch)\n",
      "    └── metadata/     (CSV files and summaries)\n",
      "\n",
      "✓ Input directory found\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "INPUT_PATH = r\"D:\\VIT\\IV-Year\\PJT-I\\Speech Imagery Decoding\\Inner_Speech_Dataset\\Dataset\\processed_epochs\"\n",
    "OUTPUT_PATH = r\"D:\\VIT\\IV-Year\\PJT-I\\Speech Imagery Decoding\\Inner_Speech_Dataset\\Dataset\\individual_channels\"\n",
    "\n",
    "\n",
    "# Create output directory structure\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_PATH}/by_epoch\", exist_ok=True)      # Organized by epoch\n",
    "os.makedirs(f\"{OUTPUT_PATH}/by_channel\", exist_ok=True)    # Organized by channel\n",
    "os.makedirs(f\"{OUTPUT_PATH}/metadata\", exist_ok=True)      # Metadata files\n",
    "\n",
    "print(f\"Input: {INPUT_PATH}\")\n",
    "print(f\"Output: {OUTPUT_PATH}\")\n",
    "print(f\"\\nOutput structure:\")\n",
    "print(f\"  {OUTPUT_PATH}/\")\n",
    "print(f\"    ├── by_epoch/     (folders per epoch, files per channel)\")\n",
    "print(f\"    ├── by_channel/   (folders per channel, files per epoch)\")\n",
    "print(f\"    └── metadata/     (CSV files and summaries)\")\n",
    "\n",
    "# Check input exists\n",
    "if not os.path.exists(INPUT_PATH):\n",
    "    print(f\"\\n❌ ERROR: {INPUT_PATH} not found!\")\n",
    "    print(\"Run the epoch processing notebook first.\")\n",
    "else:\n",
    "    print(f\"\\n✓ Input directory found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded metadata for 5440 processed epochs\n",
      "Duration: 2.50s\n",
      "Time points: 641\n",
      "Channels: 128\n",
      "Sampling frequency: 256.0 Hz\n"
     ]
    }
   ],
   "source": [
    "# Load processed metadata\n",
    "metadata_file = f\"{INPUT_PATH}/metadata/processed_epochs_metadata.pkl\"\n",
    "\n",
    "if os.path.exists(metadata_file):\n",
    "    processed_metadata = pd.read_pickle(metadata_file)\n",
    "    print(f\"Loaded metadata for {len(processed_metadata)} processed epochs\")\n",
    "    print(f\"Duration: {processed_metadata['duration_seconds'].iloc[0]:.2f}s\")\n",
    "    print(f\"Time points: {processed_metadata['n_timepoints'].iloc[0]}\")\n",
    "    print(f\"Channels: {processed_metadata['n_channels'].iloc[0]}\")\n",
    "    print(f\"Sampling frequency: {processed_metadata['sampling_frequency'].iloc[0]} Hz\")\n",
    "else:\n",
    "    print(f\"❌ ERROR: Processed metadata not found at {metadata_file}\")\n",
    "    raise FileNotFoundError(\"Run epoch processing first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel information:\n",
      "  Total channels: 128\n",
      "  Time points per channel: 641\n",
      "  Sampling frequency: 256.0 Hz\n",
      "  First 10 channels: ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10']\n",
      "  Last 10 channels: ['D23', 'D24', 'D25', 'D26', 'D27', 'D28', 'D29', 'D30', 'D31', 'D32']\n",
      "\n",
      "✓ Created 128 channel directories\n"
     ]
    }
   ],
   "source": [
    "# Get channel names from first epoch\n",
    "sample_epoch_file = processed_metadata['file_path'].iloc[0]\n",
    "sample_epoch = mne.read_epochs(sample_epoch_file, verbose=False)\n",
    "channel_names = sample_epoch.ch_names\n",
    "n_channels = len(channel_names)\n",
    "n_timepoints = sample_epoch.get_data().shape[2]\n",
    "sfreq = sample_epoch.info['sfreq']\n",
    "\n",
    "print(f\"Channel information:\")\n",
    "print(f\"  Total channels: {n_channels}\")\n",
    "print(f\"  Time points per channel: {n_timepoints}\")\n",
    "print(f\"  Sampling frequency: {sfreq} Hz\")\n",
    "print(f\"  First 10 channels: {channel_names[:10]}\")\n",
    "print(f\"  Last 10 channels: {channel_names[-10:]}\")\n",
    "\n",
    "# Create channel directories\n",
    "for channel in channel_names:\n",
    "    os.makedirs(f\"{OUTPUT_PATH}/by_channel/{channel}\", exist_ok=True)\n",
    "\n",
    "print(f\"\\n✓ Created {n_channels} channel directories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting 5440 epochs into individual channels...\n",
      "Output will be saved in: D:\\VIT\\IV-Year\\PJT-I\\Speech Imagery Decoding\\Inner_Speech_Dataset\\Dataset\\individual_channels\\by_epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting channels:   0%|          | 0/5440 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting channels: 100%|██████████| 5440/5440 [27:25<00:00,  3.31it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CHANNEL SPLITTING COMPLETE ===\n",
      "✓ Successfully processed: 5440 epochs\n",
      "✗ Failed to process:    0 epochs\n",
      "✓ Total channel files created: 696320\n",
      "► Success rate: 100.0%\n",
      "\n",
      "Created a new metadata DataFrame with 696320 entries.\n",
      "✓ Final metadata saved to: D:\\VIT\\IV-Year\\PJT-I\\Speech Imagery Decoding\\Inner_Speech_Dataset\\Dataset\\individual_channels\\channels_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "def split_epoch_channels(row, output_base_path):\n",
    "    \"\"\"\n",
    "    Splits a single epoch into individual channel files, with filenames\n",
    "    containing class and speech type. The files are saved in a directory\n",
    "    structure organized by epoch ID.\n",
    "\n",
    "    Parameters:\n",
    "    - row (pd.Series): A row from the metadata DataFrame containing all\n",
    "                       necessary info for one epoch (file_path, epoch_id,\n",
    "                       class, speech_type).\n",
    "    - output_base_path (str): The base directory where files will be saved.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing:\n",
    "        - success (bool): True if processing was successful, False otherwise.\n",
    "        - channel_files (list): A list of dictionaries, where each dictionary\n",
    "                                contains metadata about a newly created channel file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # --- Extract metadata from the row ---\n",
    "        epoch_file = row['file_path']\n",
    "        epoch_id = row['epoch_id']\n",
    "        speech_type = row['speech_type']\n",
    "        # Use 'class_label' to avoid conflict with the Python keyword 'class'\n",
    "        class_label = row['class']\n",
    "\n",
    "        # --- Load epoch data ---\n",
    "        # We read the .fif file which contains the single epoch\n",
    "        epoch = mne.read_epochs(epoch_file, verbose=False)\n",
    "        # Get the data, shape is (1, n_channels, n_timepoints)\n",
    "        # and remove the first dimension.\n",
    "        data = epoch.get_data()[0]  # Final shape: (n_channels, n_timepoints)\n",
    "\n",
    "        channel_files_metadata = []\n",
    "\n",
    "        # --- Create the output directory for this epoch ---\n",
    "        # All channels for this epoch will be saved here.\n",
    "        epoch_dir = os.path.join(output_base_path, \"by_epoch\", str(epoch_id))\n",
    "        os.makedirs(epoch_dir, exist_ok=True)\n",
    "\n",
    "        # --- Iterate through each channel, save it, and record metadata ---\n",
    "        for ch_idx, channel_name in enumerate(epoch.ch_names):\n",
    "            channel_data = data[ch_idx]  # Shape: (n_timepoints,)\n",
    "\n",
    "            # Sanitize the channel name to ensure it's a valid filename component\n",
    "            safe_channel_name = \"\".join(c for c in channel_name if c.isalnum() or c in ('-', '_')).rstrip()\n",
    "\n",
    "            # --- Define the new filename and full path ---\n",
    "            # Format: {epoch_id}_{speech_type}_{class}_{channel_name}.npy\n",
    "            file_name = f\"{epoch_id}_{speech_type}_{class_label}_{safe_channel_name}.npy\"\n",
    "            output_file_path = os.path.join(epoch_dir, file_name)\n",
    "\n",
    "            # Save the individual channel data as a .npy file\n",
    "            np.save(output_file_path, channel_data)\n",
    "\n",
    "            # --- Record metadata for the file we just created ---\n",
    "            # This information can be used to build a new master CSV file later.\n",
    "            channel_files_metadata.append({\n",
    "                'channel_index': ch_idx,\n",
    "                'channel_name': channel_name,\n",
    "                'new_file_path': output_file_path,\n",
    "                'new_file_name': file_name,\n",
    "                # Add some basic stats about the channel data\n",
    "                'data_shape': channel_data.shape,\n",
    "                'data_min': float(channel_data.min()),\n",
    "                'data_max': float(channel_data.max()),\n",
    "                'data_mean': float(channel_data.mean()),\n",
    "                'data_std': float(channel_data.std())\n",
    "            })\n",
    "\n",
    "        # Return success and the list of metadata for all processed channels\n",
    "        return True, channel_files_metadata\n",
    "\n",
    "    except Exception as e:\n",
    "        # If anything goes wrong, print the error and return failure\n",
    "        print(f\"Error processing epoch_id {row.get('epoch_id', 'N/A')} from file {row.get('file_path', 'N/A')}: {e}\")\n",
    "        return False, []\n",
    "\n",
    "# ==============================================================================\n",
    "# MAIN PROCESSING SCRIPT\n",
    "# ==============================================================================\n",
    "# Assume 'processed_metadata' is a pandas DataFrame loaded with your epoch info\n",
    "# and 'OUTPUT_PATH' is the base path for your results.\n",
    "# For example:\n",
    "# OUTPUT_PATH = \"path/to/your/output\"\n",
    "# processed_metadata = pd.read_csv(\"path/to/your/metadata.csv\")\n",
    "# n_channels = 64 # Example channel count\n",
    "\n",
    "# --- Mock data for demonstration if you don't have it loaded ---\n",
    "# Create a dummy metadata DataFrame for testing purposes\n",
    "if 'processed_metadata' not in locals():\n",
    "    print(\"Creating dummy `processed_metadata` DataFrame for demonstration.\")\n",
    "    processed_metadata = pd.DataFrame({\n",
    "        'file_path': ['dummy_epoch_1.fif', 'dummy_epoch_2.fif'],\n",
    "        'epoch_id': [101, 102],\n",
    "        'subject_name': ['sub-01', 'sub-01'],\n",
    "        'subject_number': [1, 1],\n",
    "        'session_number': [1, 1],\n",
    "        'trial_number': [1, 2],\n",
    "        'speech_type': ['imagined', 'vocalized'],\n",
    "        'class': ['left', 'right'],\n",
    "        'class_id': [0, 1],\n",
    "        'timestamp': [pd.Timestamp.now(), pd.Timestamp.now()],\n",
    "        'sampling_frequency': [512, 512],\n",
    "        'duration_seconds': [2.0, 2.0],\n",
    "        'time_start': [0.0, 0.0],\n",
    "        'time_end': [2.0, 2.0],\n",
    "        'freq_low': [1.0, 1.0],\n",
    "        'freq_high': [40.0, 40.0]\n",
    "    })\n",
    "    # Create dummy .fif files\n",
    "    n_channels = 64\n",
    "    n_times = 1024 # 2 seconds at 512 Hz\n",
    "    sfreq = 512\n",
    "    ch_names = [f'EEG {i:03}' for i in range(n_channels)]\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types='eeg')\n",
    "    dummy_data = np.random.randn(1, n_channels, n_times)\n",
    "    for f in processed_metadata['file_path']:\n",
    "        mne.EpochsArray(dummy_data, info).save(f, overwrite=True)\n",
    "    print(\"Dummy files created.\\n\")\n",
    "\n",
    "if 'OUTPUT_PATH' not in locals():\n",
    "    OUTPUT_PATH = \"./eeg_output\"\n",
    "    print(f\"Setting `OUTPUT_PATH` to '{OUTPUT_PATH}' for demonstration.\\n\")\n",
    "# --- End of mock data section ---\n",
    "\n",
    "\n",
    "print(f\"Splitting {len(processed_metadata)} epochs into individual channels...\")\n",
    "print(f\"Output will be saved in: {os.path.join(OUTPUT_PATH, 'by_epoch')}\")\n",
    "\n",
    "all_channels_metadata = []\n",
    "success_count = 0\n",
    "fail_count = 0\n",
    "\n",
    "# Use tqdm for a progress bar\n",
    "for _, row in tqdm(processed_metadata.iterrows(), total=len(processed_metadata), desc=\"Splitting channels\"):\n",
    "    success, channel_files_list = split_epoch_channels(row, OUTPUT_PATH)\n",
    "\n",
    "    if success:\n",
    "        # For each channel file created, merge the original epoch metadata\n",
    "        # with the new channel-specific metadata.\n",
    "        for channel_meta in channel_files_list:\n",
    "            # Combine original row (as a dict) with the new metadata\n",
    "            full_record = {**row.to_dict(), **channel_meta}\n",
    "            all_channels_metadata.append(full_record)\n",
    "        success_count += 1\n",
    "    else:\n",
    "        fail_count += 1\n",
    "\n",
    "print(\"\\n=== CHANNEL SPLITTING COMPLETE ===\")\n",
    "print(f\"✓ Successfully processed: {success_count} epochs\")\n",
    "print(f\"✗ Failed to process:    {fail_count} epochs\")\n",
    "if (success_count + fail_count) > 0:\n",
    "    success_rate = (success_count / (success_count + fail_count)) * 100\n",
    "    print(f\"✓ Total channel files created: {len(all_channels_metadata)}\")\n",
    "    print(f\"► Success rate: {success_rate:.1f}%\")\n",
    "\n",
    "# You can now create a new, detailed DataFrame with all the information\n",
    "if all_channels_metadata:\n",
    "    final_metadata_df = pd.DataFrame(all_channels_metadata)\n",
    "    print(f\"\\nCreated a new metadata DataFrame with {len(final_metadata_df)} entries.\")\n",
    "    # Save the final metadata to a CSV file for future use\n",
    "    final_csv_path = os.path.join(OUTPUT_PATH, \"channels_metadata.csv\")\n",
    "    final_metadata_df.to_csv(final_csv_path, index=False)\n",
    "    print(f\"✓ Final metadata saved to: {final_csv_path}\")\n",
    "    # Display the first few rows of the new DataFrame\n",
    "    # display(final_metadata_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CHANNEL SPLITTING RESULTS ===\n",
      "Total channel files: 696320\n",
      "Unique epochs: 5440\n",
      "Unique channels: 128\n",
      "Files per epoch: 128\n",
      "Files per channel: 5440\n",
      "\n",
      "=== DATA STATISTICS ===\n",
      "Data shape per channel: (641,)\n",
      "Data range: -6.03e-04 to 5.37e-04\n",
      "Average mean: -4.81e-10\n",
      "Average std: 4.66e-06\n",
      "\n",
      "=== DISTRIBUTION CHECK ===\n",
      "Files per speech type:\n",
      "  Visualized: 281088 files (2196 epochs)\n",
      "  Inner: 275968 files (2156 epochs)\n",
      "  Pronounced: 139264 files (1088 epochs)\n",
      "\n",
      "Files per class:\n",
      "  Left: 174080 files (1360 epochs)\n",
      "  Up: 174080 files (1360 epochs)\n",
      "  Right: 174080 files (1360 epochs)\n",
      "  Down: 174080 files (1360 epochs)\n",
      "\n",
      "Files per subject:\n",
      "  Subject 1: 64000 files (500 epochs)\n",
      "  Subject 2: 76800 files (600 epochs)\n",
      "  Subject 3: 64000 files (500 epochs)\n",
      "  Subject 4: 76800 files (600 epochs)\n",
      "  Subject 5: 76800 files (600 epochs)\n"
     ]
    }
   ],
   "source": [
    "# Analyze results\n",
    "if len(all_channels_metadata) > 0:\n",
    "    channel_df = pd.DataFrame(all_channels_metadata)\n",
    "    \n",
    "    print(\"=== CHANNEL SPLITTING RESULTS ===\")\n",
    "    print(f\"Total channel files: {len(channel_df)}\")\n",
    "    print(f\"Unique epochs: {channel_df['epoch_id'].nunique()}\")\n",
    "    print(f\"Unique channels: {channel_df['channel_name'].nunique()}\")\n",
    "    print(f\"Files per epoch: {len(channel_df) // channel_df['epoch_id'].nunique()}\")\n",
    "    print(f\"Files per channel: {len(channel_df) // channel_df['channel_name'].nunique()}\")\n",
    "    \n",
    "    print(\"\\n=== DATA STATISTICS ===\")\n",
    "    print(f\"Data shape per channel: {channel_df['data_shape'].iloc[0]}\")\n",
    "    print(f\"Data range: {channel_df['data_min'].min():.2e} to {channel_df['data_max'].max():.2e}\")\n",
    "    print(f\"Average mean: {channel_df['data_mean'].mean():.2e}\")\n",
    "    print(f\"Average std: {channel_df['data_std'].mean():.2e}\")\n",
    "    \n",
    "    print(\"\\n=== DISTRIBUTION CHECK ===\")\n",
    "    print(\"Files per speech type:\")\n",
    "    speech_counts = channel_df['speech_type'].value_counts()\n",
    "    for speech_type, count in speech_counts.items():\n",
    "        print(f\"  {speech_type}: {count} files ({count//n_channels} epochs)\")\n",
    "    \n",
    "    print(\"\\nFiles per class:\")\n",
    "    class_counts = channel_df['class'].value_counts()\n",
    "    for class_name, count in class_counts.items():\n",
    "        print(f\"  {class_name}: {count} files ({count//n_channels} epochs)\")\n",
    "    \n",
    "    print(\"\\nFiles per subject:\")\n",
    "    subject_counts = channel_df['subject_number'].value_counts().sort_index()\n",
    "    for subject, count in subject_counts.head(5).items():\n",
    "        print(f\"  Subject {subject}: {count} files ({count//n_channels} epochs)\")\n",
    "    \n",
    "   \n",
    "else:\n",
    "    print(\"❌ No channel files were created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TESTING CHANNEL FILES ===\n",
      "\n",
      "Test 1: sub-01_ses-01_trial_000 - A1\n",
      "  ✗ Error loading files: 'by_epoch_path'\n",
      "\n",
      "Test 2: sub-01_ses-01_trial_000 - A2\n",
      "  ✗ Error loading files: 'by_epoch_path'\n",
      "\n",
      "Test 3: sub-01_ses-01_trial_000 - A3\n",
      "  ✗ Error loading files: 'by_epoch_path'\n",
      "\n",
      "=== TESTING CHANNEL COLLECTION ===\n",
      "Testing channel 'A1' with 5440 files\n",
      "  File 1: Error - 'by_channel_path'\n",
      "  File 2: Error - 'by_channel_path'\n",
      "  File 3: Error - 'by_channel_path'\n"
     ]
    }
   ],
   "source": [
    "# Test loading individual channel files\n",
    "if len(all_channels_metadata) > 0:\n",
    "    print(\"=== TESTING CHANNEL FILES ===\")\n",
    "    \n",
    "    # Test first 3 channel files\n",
    "    test_files = channel_df.head(3)\n",
    "    \n",
    "    for i, (_, row) in enumerate(test_files.iterrows()):\n",
    "        print(f\"\\nTest {i+1}: {row['epoch_id']} - {row['channel_name']}\")\n",
    "        \n",
    "        try:\n",
    "            # Test by_epoch file\n",
    "            data_epoch = np.load(row['by_epoch_path'])\n",
    "            print(f\"  ✓ by_epoch file loaded: {data_epoch.shape}\")\n",
    "            \n",
    "            # Test by_channel file\n",
    "            data_channel = np.load(row['by_channel_path'])\n",
    "            print(f\"  ✓ by_channel file loaded: {data_channel.shape}\")\n",
    "            \n",
    "            # Verify they're identical\n",
    "            if np.array_equal(data_epoch, data_channel):\n",
    "                print(f\"  ✓ Files are identical\")\n",
    "            else:\n",
    "                print(f\"  ⚠ Files differ!\")\n",
    "            \n",
    "            print(f\"  Data range: {data_epoch.min():.2e} to {data_epoch.max():.2e}\")\n",
    "            print(f\"  Duration: {len(data_epoch) / row['sampling_frequency']:.2f}s\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error loading files: {e}\")\n",
    "            \n",
    "    # Test loading all files for one channel\n",
    "    print(f\"\\n=== TESTING CHANNEL COLLECTION ===\")\n",
    "    test_channel = channel_names[0]  # First channel\n",
    "    channel_files = channel_df[channel_df['channel_name'] == test_channel]\n",
    "    \n",
    "    print(f\"Testing channel '{test_channel}' with {len(channel_files)} files\")\n",
    "    \n",
    "    # Load first 3 files for this channel\n",
    "    for i, (_, row) in enumerate(channel_files.head(3).iterrows()):\n",
    "        try:\n",
    "            data = np.load(row['by_channel_path'])\n",
    "            print(f\"  File {i+1}: {row['epoch_id']} - Shape: {data.shape}, Range: {data.min():.2e} to {data.max():.2e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  File {i+1}: Error - {e}\")\n",
    "            \n",
    "else:\n",
    "    print(\"No channel files to test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FILES SAVED ===\n",
      "✓ 696320 channel files (.npy format)\n",
      "✓ Metadata: D:\\VIT\\IV-Year\\PJT-I\\Speech Imagery Decoding\\Inner_Speech_Dataset\\Dataset\\individual_channels/metadata/channel_files_metadata.csv\n",
      "✓ Metadata: D:\\VIT\\IV-Year\\PJT-I\\Speech Imagery Decoding\\Inner_Speech_Dataset\\Dataset\\individual_channels/metadata/channel_files_metadata.pkl\n",
      "✓ Summary: D:\\VIT\\IV-Year\\PJT-I\\Speech Imagery Decoding\\Inner_Speech_Dataset\\Dataset\\individual_channels/metadata/split_summary.txt\n",
      "\n",
      "=== USAGE EXAMPLES ===\n",
      "\n",
      "1. Load single channel file:\n",
      "   data = np.load('individual_channels/by_epoch/epoch_id/channel.npy')\n",
      "   # Shape: (641,) - time series for one channel\n",
      "\n",
      "2. Load all channels for one epoch:\n",
      "   epoch_folder = 'individual_channels/by_epoch/sub-01_ses-01_trial_000_Inner_Up/'\n",
      "   channels = {}\n",
      "   for ch in channel_names:\n",
      "       channels[ch] = np.load(f'{epoch_folder}/{ch}.npy')\n",
      "\n",
      "3. Load all epochs for one channel:\n",
      "   channel_folder = 'individual_channels/by_channel/A1/'\n",
      "   epochs = []\n",
      "   for file in os.listdir(channel_folder):\n",
      "       epochs.append(np.load(f'{channel_folder}/{file}'))\n",
      "\n",
      "4. Use metadata for filtering:\n",
      "   metadata = pd.read_csv('D:\\VIT\\IV-Year\\PJT-I\\Speech Imagery Decoding\\Inner_Speech_Dataset\\Dataset\\individual_channels/metadata/channel_files_metadata.csv')\n",
      "   inner_speech_A1 = metadata[\n",
      "       (metadata['speech_type'] == 'Inner') & \n",
      "       (metadata['channel_name'] == 'A1')\n",
      "   ]\n",
      "\n",
      "🎉 CHANNEL SPLITTING COMPLETE!\n",
      "Your data is now organized as:\n",
      "  • 696320 individual channel files\n",
      "  • 5440 epochs × 128 channels\n",
      "  • Organized by epoch AND by channel\n",
      "  • Each file: 641 time points, 2.50s duration\n",
      "  • Ready for channel-specific analysis in: D:\\VIT\\IV-Year\\PJT-I\\Speech Imagery Decoding\\Inner_Speech_Dataset\\Dataset\\individual_channels\n"
     ]
    }
   ],
   "source": [
    "# Save metadata and create summary\n",
    "if len(all_channels_metadata) > 0:\n",
    "    # Save channel files metadata\n",
    "    csv_file = f\"{OUTPUT_PATH}/metadata/channel_files_metadata.csv\"\n",
    "    pkl_file = f\"{OUTPUT_PATH}/metadata/channel_files_metadata.pkl\"\n",
    "    summary_file = f\"{OUTPUT_PATH}/metadata/split_summary.txt\"\n",
    "    \n",
    "    channel_df.to_csv(csv_file, index=False)\n",
    "    channel_df.to_pickle(pkl_file)\n",
    "    \n",
    "    # Create summary\n",
    "    with open(summary_file, 'w') as f:\n",
    "        f.write(\"CHANNEL SPLITTING SUMMARY\\n\")\n",
    "        f.write(\"=\" * 40 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Input: {INPUT_PATH}\\n\")\n",
    "        f.write(f\"Output: {OUTPUT_PATH}\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Results:\\n\")\n",
    "        f.write(f\"  Successfully processed epochs: {success_count}\\n\")\n",
    "        f.write(f\"  Failed epochs: {fail_count}\\n\")\n",
    "        f.write(f\"  Total channel files created: {len(channel_df)}\\n\")\n",
    "        f.write(f\"  Success rate: {success_count/(success_count+fail_count)*100:.1f}%\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Dataset Info:\\n\")\n",
    "        f.write(f\"  Unique epochs: {channel_df['epoch_id'].nunique()}\\n\")\n",
    "        f.write(f\"  Unique channels: {channel_df['channel_name'].nunique()}\\n\")\n",
    "        f.write(f\"  Files per epoch: {len(channel_df) // channel_df['epoch_id'].nunique()}\\n\")\n",
    "        f.write(f\"  Files per channel: {len(channel_df) // channel_df['channel_name'].nunique()}\\n\")\n",
    "        f.write(f\"  Data shape per file: {channel_df['data_shape'].iloc[0]}\\n\")\n",
    "        f.write(f\"  Sampling frequency: {channel_df['sampling_frequency'].iloc[0]} Hz\\n\")\n",
    "        f.write(f\"  Duration per file: {channel_df['duration_seconds'].iloc[0]} seconds\\n\\n\")\n",
    "        \n",
    "        f.write(f\"File Organization:\\n\")\n",
    "        f.write(f\"  by_epoch/: {channel_df['epoch_id'].nunique()} folders, {n_channels} files each\\n\")\n",
    "        f.write(f\"  by_channel/: {n_channels} folders, {channel_df['epoch_id'].nunique()} files each\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Speech Type Distribution:\\n\")\n",
    "        for speech_type, count in channel_df['speech_type'].value_counts().items():\n",
    "            f.write(f\"  {speech_type}: {count} files ({count//n_channels} epochs)\\n\")\n",
    "        \n",
    "        f.write(f\"\\nClass Distribution:\\n\")\n",
    "        for class_name, count in channel_df['class'].value_counts().items():\n",
    "            f.write(f\"  {class_name}: {count} files ({count//n_channels} epochs)\\n\")\n",
    "        \n",
    "        f.write(f\"\\nChannel Names:\\n\")\n",
    "        for i, channel in enumerate(channel_names):\n",
    "            if i < 10 or i >= len(channel_names) - 10:\n",
    "                f.write(f\"  {channel}\\n\")\n",
    "            elif i == 10:\n",
    "                f.write(f\"  ... ({len(channel_names) - 20} more channels) ...\\n\")\n",
    "    \n",
    "    print(f\"\\n=== FILES SAVED ===\")\n",
    "    print(f\"✓ {len(channel_df)} channel files (.npy format)\")\n",
    "    print(f\"✓ Metadata: {csv_file}\")\n",
    "    print(f\"✓ Metadata: {pkl_file}\")\n",
    "    print(f\"✓ Summary: {summary_file}\")\n",
    "    \n",
    "    print(f\"\\n=== USAGE EXAMPLES ===\")\n",
    "    print(f\"\\n1. Load single channel file:\")\n",
    "    print(f\"   data = np.load('individual_channels/by_epoch/epoch_id/channel.npy')\")\n",
    "    print(f\"   # Shape: ({n_timepoints},) - time series for one channel\")\n",
    "    \n",
    "    print(f\"\\n2. Load all channels for one epoch:\")\n",
    "    print(f\"   epoch_folder = 'individual_channels/by_epoch/sub-01_ses-01_trial_000_Inner_Up/'\")\n",
    "    print(f\"   channels = {{}}\")\n",
    "    print(f\"   for ch in channel_names:\")\n",
    "    print(f\"       channels[ch] = np.load(f'{{epoch_folder}}/{{ch}}.npy')\")\n",
    "    \n",
    "    print(f\"\\n3. Load all epochs for one channel:\")\n",
    "    print(f\"   channel_folder = 'individual_channels/by_channel/A1/'\")\n",
    "    print(f\"   epochs = []\")\n",
    "    print(f\"   for file in os.listdir(channel_folder):\")\n",
    "    print(f\"       epochs.append(np.load(f'{{channel_folder}}/{{file}}'))\")\n",
    "    \n",
    "    print(f\"\\n4. Use metadata for filtering:\")\n",
    "    print(f\"   metadata = pd.read_csv('{csv_file}')\")\n",
    "    print(f\"   inner_speech_A1 = metadata[\")\n",
    "    print(f\"       (metadata['speech_type'] == 'Inner') & \")\n",
    "    print(f\"       (metadata['channel_name'] == 'A1')\")\n",
    "    print(f\"   ]\")\n",
    "    \n",
    "    print(f\"\\n🎉 CHANNEL SPLITTING COMPLETE!\")\n",
    "    print(f\"Your data is now organized as:\")\n",
    "    print(f\"  • {len(channel_df)} individual channel files\")\n",
    "    print(f\"  • {channel_df['epoch_id'].nunique()} epochs × {n_channels} channels\")\n",
    "    print(f\"  • Organized by epoch AND by channel\")\n",
    "    print(f\"  • Each file: {n_timepoints} time points, {channel_df['duration_seconds'].iloc[0]:.2f}s duration\")\n",
    "    print(f\"  • Ready for channel-specific analysis in: {OUTPUT_PATH}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No channel files to save\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
