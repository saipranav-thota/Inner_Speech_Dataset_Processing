{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Epoch Extraction - Individual MNE Files\n",
    "\n",
    "This notebook extracts individual epochs from the Inner Speech EEG dataset and saves each epoch as:\n",
    "- Individual MNE Epochs object (.fif files)\n",
    "- Complete metadata CSV/pickle files\n",
    "- Organized folder structure by subject/session\n",
    "\n",
    "Each epoch will be saved as a separate MNE file that can be loaded directly with `mne.read_epochs()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "# Import custom processing functions\n",
    "from Python_Processing.Data_extractions import (\n",
    "    extract_block_data_from_subject,\n",
    "    load_events\n",
    ")\n",
    "from Python_Processing.Utilitys import sub_name\n",
    "\n",
    "# Set MNE logging level to reduce output\n",
    "mne.set_log_level('WARNING')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset path: dataset\n",
      "Output path: extracted_epochs_mne\n",
      "Subjects: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Sessions: [1, 2, 3]\n",
      "\n",
      "Output structure:\n",
      "  extracted_epochs_mne/\n",
      "    ├── individual_epochs/  (MNE .fif files)\n",
      "    └── metadata/          (CSV and pickle files)\n"
     ]
    }
   ],
   "source": [
    "# Dataset configuration\n",
    "derivatives_path = \"dataset\"  # Path to your dataset folder\n",
    "output_path = \"extracted_epochs_mne\"  # Output folder for extracted epochs\n",
    "\n",
    "# Create output directory structure\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "os.makedirs(os.path.join(output_path, \"individual_epochs\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_path, \"metadata\"), exist_ok=True)\n",
    "\n",
    "# Define mappings\n",
    "class_names = {0: 'Up', 1: 'Down', 2: 'Right', 3: 'Left'}\n",
    "condition_names = {0: 'Pronounced', 1: 'Inner', 2: 'Visualized'}\n",
    "\n",
    "# Subject and session ranges\n",
    "subjects = list(range(1, 11))  # Subjects 1-10\n",
    "sessions = [1, 2, 3]  # Sessions 1-3\n",
    "\n",
    "print(f\"Dataset path: {derivatives_path}\")\n",
    "print(f\"Output path: {output_path}\")\n",
    "print(f\"Subjects: {subjects}\")\n",
    "print(f\"Sessions: {sessions}\")\n",
    "print(f\"\\nOutput structure:\")\n",
    "print(f\"  {output_path}/\")\n",
    "print(f\"    ├── individual_epochs/  (MNE .fif files)\")\n",
    "print(f\"    └── metadata/          (CSV and pickle files)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Individual Epochs as MNE Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch extraction as MNE files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subjects/sessions:  53%|█████▎    | 16/30 [04:34<03:58, 17.06s/it, Current=sub-06/ses-01, Epochs=200, Total=3000]d:\\VIT\\IV-Year\\PJT-I\\Speech Imagery Decoding\\Inner_Speech_Dataset\\Python_Processing\\Data_extractions.py:124: RuntimeWarning: Invalid tag with only 0/16 bytes at position 0\n",
      "  X = mne.read_epochs(file_name, verbose='WARNING')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error processing sub-06/ses-02: 'NoneType' object has no attribute 'kind'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subjects/sessions:  97%|█████████▋| 29/30 [09:48<00:20, 20.30s/it, Current=sub-10/ses-03, Epochs=200, Total=5440]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extraction complete!\n",
      "Total epochs extracted: 5440\n",
      "Total subjects: 10\n",
      "Total sessions per subject: 3\n",
      "Individual MNE files saved to: extracted_epochs_mne\\individual_epochs\n"
     ]
    }
   ],
   "source": [
    "def extract_epochs_as_mne_files(derivatives_path, subjects, sessions, output_path):\n",
    "    \"\"\"\n",
    "    Extract individual epochs and save each as a separate MNE Epochs file.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with epoch metadata including file paths\n",
    "    \"\"\"\n",
    "    \n",
    "    all_epochs_metadata = []\n",
    "    total_epochs = 0\n",
    "    \n",
    "    # Progress tracking\n",
    "    total_combinations = len(subjects) * len(sessions)\n",
    "    pbar = tqdm(total=total_combinations, desc=\"Processing subjects/sessions\")\n",
    "    \n",
    "    for subject_num in subjects:\n",
    "        subject_id = f\"sub-{subject_num:02d}\"\n",
    "        \n",
    "        # Create subject directory\n",
    "        subject_dir = os.path.join(output_path, \"individual_epochs\", subject_id)\n",
    "        os.makedirs(subject_dir, exist_ok=True)\n",
    "        \n",
    "        for session_num in sessions:\n",
    "            session_id = f\"ses-{session_num:02d}\"\n",
    "            \n",
    "            # Create session directory\n",
    "            session_dir = os.path.join(subject_dir, session_id)\n",
    "            os.makedirs(session_dir, exist_ok=True)\n",
    "            \n",
    "            try:\n",
    "                # Load EEG data and events for this subject/session\n",
    "                X_session, Y_session = extract_block_data_from_subject(\n",
    "                    derivatives_path, subject_num, \"eeg\", session_num\n",
    "                )\n",
    "                \n",
    "                # Get sampling frequency and channel names\n",
    "                sfreq = X_session.info['sfreq']\n",
    "                ch_names = X_session.ch_names\n",
    "                info = X_session.info.copy()\n",
    "                \n",
    "                # Process each epoch\n",
    "                for trial_idx in range(len(X_session)):\n",
    "                    # Extract event information\n",
    "                    timestamp = Y_session[trial_idx, 0]\n",
    "                    class_id = int(Y_session[trial_idx, 1])\n",
    "                    condition_id = int(Y_session[trial_idx, 2])\n",
    "                    session_from_events = int(Y_session[trial_idx, 3])\n",
    "                    \n",
    "                    # Create unique epoch ID\n",
    "                    epoch_id = f\"{subject_id}_{session_id}_trial_{trial_idx:03d}\"\n",
    "                    \n",
    "                    # Create filename for this epoch\n",
    "                    epoch_filename = f\"{epoch_id}_{condition_names[condition_id]}_{class_names[class_id]}-epo.fif\"\n",
    "                    epoch_filepath = os.path.join(session_dir, epoch_filename)\n",
    "                    \n",
    "                    # Extract single epoch data\n",
    "                    single_epoch_data = X_session._data[trial_idx:trial_idx+1]  # Keep as (1, n_channels, n_timepoints)\n",
    "                    \n",
    "                    # Create events array for this single epoch\n",
    "                    # MNE expects events as (n_events, 3) with [sample, prev_id, event_id]\n",
    "                    single_epoch_events = np.array([[0, 0, class_id + 1]])  # +1 because MNE event IDs should be > 0\n",
    "                    \n",
    "                    # Create event_id dictionary\n",
    "                    event_id = {class_names[class_id]: class_id + 1}\n",
    "                    \n",
    "                    # Create MNE Epochs object for single epoch\n",
    "                    single_epoch = mne.EpochsArray(\n",
    "                        single_epoch_data,\n",
    "                        info,\n",
    "                        events=single_epoch_events,\n",
    "                        event_id=event_id,\n",
    "                        tmin=X_session.tmin,\n",
    "                        verbose=False\n",
    "                    )\n",
    "                    \n",
    "                    # Add metadata to the epoch\n",
    "                    epoch_metadata_dict = {\n",
    "                        'subject_id': subject_id,\n",
    "                        'subject_number': subject_num,\n",
    "                        'session_id': session_id,\n",
    "                        'session_number': session_num,\n",
    "                        'trial_number': trial_idx,\n",
    "                        'speech_type': condition_names[condition_id],\n",
    "                        'condition_id': condition_id,\n",
    "                        'class': class_names[class_id],\n",
    "                        'class_id': class_id,\n",
    "                        'timestamp': timestamp\n",
    "                    }\n",
    "                    \n",
    "                    # Convert to DataFrame for MNE metadata\n",
    "                    single_epoch.metadata = pd.DataFrame([epoch_metadata_dict])\n",
    "                    \n",
    "                    # Save the single epoch as MNE file\n",
    "                    single_epoch.save(epoch_filepath, overwrite=True, verbose=False)\n",
    "                    \n",
    "                    # Store epoch metadata for our master list\n",
    "                    epoch_metadata = {\n",
    "                        'epoch_id': epoch_id,\n",
    "                        'subject_name': subject_id,\n",
    "                        'subject_number': subject_num,\n",
    "                        'session_number': session_num,\n",
    "                        'session_id': session_id,\n",
    "                        'trial_number': trial_idx,\n",
    "                        'speech_type': condition_names[condition_id],\n",
    "                        'condition_id': condition_id,\n",
    "                        'class': class_names[class_id],\n",
    "                        'class_id': class_id,\n",
    "                        'timestamp': timestamp,\n",
    "                        'sampling_frequency': sfreq,\n",
    "                        'n_channels': len(ch_names),\n",
    "                        'n_timepoints': single_epoch_data.shape[2],\n",
    "                        'duration_seconds': single_epoch_data.shape[2] / sfreq,\n",
    "                        'file_path': epoch_filepath,\n",
    "                        'relative_path': os.path.relpath(epoch_filepath, output_path),\n",
    "                        'filename': epoch_filename\n",
    "                    }\n",
    "                    \n",
    "                    all_epochs_metadata.append(epoch_metadata)\n",
    "                    total_epochs += 1\n",
    "                \n",
    "                pbar.set_postfix({\n",
    "                    'Current': f\"{subject_id}/{session_id}\",\n",
    "                    'Epochs': f\"{len(X_session)}\",\n",
    "                    'Total': total_epochs\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing {subject_id}/{session_id}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            pbar.update(1)\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    epochs_df = pd.DataFrame(all_epochs_metadata)\n",
    "    \n",
    "    print(f\"\\nExtraction complete!\")\n",
    "    print(f\"Total epochs extracted: {total_epochs}\")\n",
    "    print(f\"Total subjects: {len(subjects)}\")\n",
    "    print(f\"Total sessions per subject: {len(sessions)}\")\n",
    "    print(f\"Individual MNE files saved to: {os.path.join(output_path, 'individual_epochs')}\")\n",
    "    \n",
    "    return epochs_df\n",
    "\n",
    "# Run the extraction\n",
    "print(\"Starting epoch extraction as MNE files...\")\n",
    "epochs_df = extract_epochs_as_mne_files(\n",
    "    derivatives_path, subjects, sessions, output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Extracted Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATASET OVERVIEW ===\n",
      "Total epochs: 5440\n",
      "Unique subjects: 10\n",
      "Sessions per subject: 3\n",
      "Sampling frequency: 256.0 Hz\n",
      "Number of channels: 128\n",
      "Epoch duration: 4.50 seconds\n",
      "Time points per epoch: 1153\n",
      "\n",
      "=== DISTRIBUTION BY SPEECH TYPE ===\n",
      "Visualized: 2196 epochs (40.4%)\n",
      "Inner: 2156 epochs (39.6%)\n",
      "Pronounced: 1088 epochs (20.0%)\n",
      "\n",
      "=== DISTRIBUTION BY CLASS ===\n",
      "Left: 1360 epochs (25.0%)\n",
      "Up: 1360 epochs (25.0%)\n",
      "Right: 1360 epochs (25.0%)\n",
      "Down: 1360 epochs (25.0%)\n",
      "\n",
      "=== EPOCHS PER SUBJECT ===\n",
      "sub-01: 500 epochs\n",
      "sub-02: 600 epochs\n",
      "sub-03: 500 epochs\n",
      "sub-04: 600 epochs\n",
      "sub-05: 600 epochs\n",
      "sub-06: 340 epochs\n",
      "sub-07: 600 epochs\n",
      "sub-08: 500 epochs\n",
      "sub-09: 600 epochs\n",
      "sub-10: 600 epochs\n",
      "\n",
      "=== FILE STRUCTURE SAMPLE ===\n",
      "Sample file paths:\n",
      "  1. individual_epochs\\sub-01\\ses-01\\sub-01_ses-01_trial_000_Pronounced_Left-epo.fif\n",
      "  2. individual_epochs\\sub-01\\ses-01\\sub-01_ses-01_trial_001_Pronounced_Up-epo.fif\n",
      "  3. individual_epochs\\sub-01\\ses-01\\sub-01_ses-01_trial_002_Pronounced_Left-epo.fif\n",
      "  4. individual_epochs\\sub-01\\ses-01\\sub-01_ses-01_trial_003_Pronounced_Left-epo.fif\n",
      "  5. individual_epochs\\sub-01\\ses-01\\sub-01_ses-01_trial_004_Pronounced_Right-epo.fif\n"
     ]
    }
   ],
   "source": [
    "# Display basic statistics\n",
    "print(\"=== DATASET OVERVIEW ===\")\n",
    "print(f\"Total epochs: {len(epochs_df)}\")\n",
    "print(f\"Unique subjects: {epochs_df['subject_number'].nunique()}\")\n",
    "print(f\"Sessions per subject: {epochs_df.groupby('subject_number')['session_number'].nunique().iloc[0]}\")\n",
    "print(f\"Sampling frequency: {epochs_df['sampling_frequency'].iloc[0]} Hz\")\n",
    "print(f\"Number of channels: {epochs_df['n_channels'].iloc[0]}\")\n",
    "print(f\"Epoch duration: {epochs_df['duration_seconds'].iloc[0]:.2f} seconds\")\n",
    "print(f\"Time points per epoch: {epochs_df['n_timepoints'].iloc[0]}\")\n",
    "\n",
    "print(\"\\n=== DISTRIBUTION BY SPEECH TYPE ===\")\n",
    "speech_type_counts = epochs_df['speech_type'].value_counts()\n",
    "for speech_type, count in speech_type_counts.items():\n",
    "    print(f\"{speech_type}: {count} epochs ({count/len(epochs_df)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n=== DISTRIBUTION BY CLASS ===\")\n",
    "class_counts = epochs_df['class'].value_counts()\n",
    "for class_name, count in class_counts.items():\n",
    "    print(f\"{class_name}: {count} epochs ({count/len(epochs_df)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n=== EPOCHS PER SUBJECT ===\")\n",
    "subject_counts = epochs_df['subject_name'].value_counts().sort_index()\n",
    "for subject, count in subject_counts.items():\n",
    "    print(f\"{subject}: {count} epochs\")\n",
    "\n",
    "print(\"\\n=== FILE STRUCTURE SAMPLE ===\")\n",
    "print(\"Sample file paths:\")\n",
    "for i, filepath in enumerate(epochs_df['relative_path'].head(5)):\n",
    "    print(f\"  {i+1}. {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Metadata Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata saved to: extracted_epochs_mne\\metadata\\epochs_metadata.csv\n",
      "Metadata (pickle) saved to: extracted_epochs_mne\\metadata\\epochs_metadata.pkl\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode characters in position 4-6: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile Structure:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m    ├── individual_epochs/\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    │   ├── sub-01/\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    │   │   ├── ses-01/\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\encodings\\cp1252.py:19\u001b[0m, in \u001b[0;36mIncrementalEncoder.encode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharmap_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode characters in position 4-6: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "# Save metadata as CSV\n",
    "metadata_file = os.path.join(output_path, \"metadata\", \"epochs_metadata.csv\")\n",
    "epochs_df.to_csv(metadata_file, index=False)\n",
    "print(f\"Metadata saved to: {metadata_file}\")\n",
    "\n",
    "# Save metadata as pickle for easier loading\n",
    "metadata_pkl_file = os.path.join(output_path, \"metadata\", \"epochs_metadata.pkl\")\n",
    "epochs_df.to_pickle(metadata_pkl_file)\n",
    "print(f\"Metadata (pickle) saved to: {metadata_pkl_file}\")\n",
    "\n",
    "# Save a summary report\n",
    "summary_file = os.path.join(output_path, \"metadata\", \"extraction_summary.txt\")\n",
    "with open(summary_file, 'w') as f:\n",
    "    f.write(\"Inner Speech EEG Dataset - Individual MNE Epochs Extraction Summary\\n\")\n",
    "    f.write(\"=\" * 70 + \"\\n\\n\")\n",
    "    f.write(f\"Total epochs extracted: {len(epochs_df)}\\n\")\n",
    "    f.write(f\"Number of subjects: {epochs_df['subject_number'].nunique()}\\n\")\n",
    "    f.write(f\"Sessions per subject: {epochs_df.groupby('subject_number')['session_number'].nunique().iloc[0]}\\n\")\n",
    "    f.write(f\"Sampling frequency: {epochs_df['sampling_frequency'].iloc[0]} Hz\\n\")\n",
    "    f.write(f\"Number of channels: {epochs_df['n_channels'].iloc[0]}\\n\")\n",
    "    f.write(f\"Epoch duration: {epochs_df['duration_seconds'].iloc[0]:.2f} seconds\\n\")\n",
    "    f.write(f\"Time points per epoch: {epochs_df['n_timepoints'].iloc[0]}\\n\\n\")\n",
    "    \n",
    "    f.write(\"File Structure:\\n\")\n",
    "    f.write(f\"  {output_path}/\\n\")\n",
    "    f.write(f\"    ├── individual_epochs/\\n\")\n",
    "    f.write(f\"    │   ├── sub-01/\\n\")\n",
    "    f.write(f\"    │   │   ├── ses-01/\\n\")\n",
    "    f.write(f\"    │   │   │   ├── sub-01_ses-01_trial_000_Inner_Up-epo.fif\\n\")\n",
    "    f.write(f\"    │   │   │   ├── sub-01_ses-01_trial_001_Inner_Down-epo.fif\\n\")\n",
    "    f.write(f\"    │   │   │   └── ...\\n\")\n",
    "    f.write(f\"    │   │   ├── ses-02/\\n\")\n",
    "    f.write(f\"    │   │   └── ses-03/\\n\")\n",
    "    f.write(f\"    │   ├── sub-02/\\n\")\n",
    "    f.write(f\"    │   └── ...\\n\")\n",
    "    f.write(f\"    └── metadata/\\n\")\n",
    "    f.write(f\"        ├── epochs_metadata.csv\\n\")\n",
    "    f.write(f\"        ├── epochs_metadata.pkl\\n\")\n",
    "    f.write(f\"        └── extraction_summary.txt\\n\\n\")\n",
    "    \n",
    "    f.write(\"Speech Type Distribution:\\n\")\n",
    "    for speech_type, count in speech_type_counts.items():\n",
    "        f.write(f\"  {speech_type}: {count} epochs ({count/len(epochs_df)*100:.1f}%)\\n\")\n",
    "    \n",
    "    f.write(\"\\nClass Distribution:\\n\")\n",
    "    for class_name, count in class_counts.items():\n",
    "        f.write(f\"  {class_name}: {count} epochs ({count/len(epochs_df)*100:.1f}%)\\n\")\n",
    "    \n",
    "    f.write(\"\\nHow to Load Individual Epochs:\\n\")\n",
    "    f.write(\"  import mne\\n\")\n",
    "    f.write(\"  epoch = mne.read_epochs('path/to/epoch-epo.fif')\\n\")\n",
    "    f.write(\"  data = epoch.get_data()  # Shape: (1, n_channels, n_timepoints)\\n\")\n",
    "    f.write(\"  metadata = epoch.metadata  # Pandas DataFrame with epoch info\\n\")\n",
    "\n",
    "print(f\"Summary report saved to: {summary_file}\")\n",
    "\n",
    "print(f\"\\n=== EXTRACTION COMPLETE ===\")\n",
    "print(f\"All files saved to: {output_path}\")\n",
    "print(f\"\\nFiles created:\")\n",
    "print(f\"  - {len(epochs_df)} individual MNE epoch files (.fif)\")\n",
    "print(f\"  - {metadata_file} (CSV format)\")\n",
    "print(f\"  - {metadata_pkl_file} (Pickle format)\")\n",
    "print(f\"  - {summary_file} (Summary report)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Loading Individual Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loading a few individual epochs\n",
    "print(\"=== TESTING INDIVIDUAL EPOCH LOADING ===\")\n",
    "\n",
    "# Load first 3 epochs as examples\n",
    "test_epochs = epochs_df.head(3)\n",
    "\n",
    "for i, (_, row) in enumerate(test_epochs.iterrows()):\n",
    "    print(f\"\\n--- Test Epoch {i+1} ---\")\n",
    "    print(f\"Epoch ID: {row['epoch_id']}\")\n",
    "    print(f\"File: {row['filename']}\")\n",
    "    print(f\"Subject: {row['subject_name']}, Session: {row['session_number']}\")\n",
    "    print(f\"Speech Type: {row['speech_type']}, Class: {row['class']}\")\n",
    "    \n",
    "    # Load the epoch\n",
    "    try:\n",
    "        epoch = mne.read_epochs(row['file_path'], verbose=False)\n",
    "        \n",
    "        print(f\"✓ Successfully loaded epoch\")\n",
    "        print(f\"  Data shape: {epoch.get_data().shape}\")\n",
    "        print(f\"  Sampling frequency: {epoch.info['sfreq']} Hz\")\n",
    "        print(f\"  Number of channels: {len(epoch.ch_names)}\")\n",
    "        print(f\"  Time range: {epoch.tmin:.2f} to {epoch.tmax:.2f} seconds\")\n",
    "        \n",
    "        # Check metadata\n",
    "        if epoch.metadata is not None:\n",
    "            print(f\"  Metadata columns: {list(epoch.metadata.columns)}\")\n",
    "            print(f\"  Metadata values: {dict(epoch.metadata.iloc[0])}\")\n",
    "        \n",
    "        # Check events\n",
    "        print(f\"  Events: {epoch.events}\")\n",
    "        print(f\"  Event ID: {epoch.event_id}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading epoch: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Usage Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== USAGE EXAMPLES ===\")\n",
    "\n",
    "# Example 1: Load all Inner speech epochs for a specific subject\n",
    "print(\"\\n1. Loading all Inner speech epochs for subject 1:\")\n",
    "inner_speech_sub1 = epochs_df[\n",
    "    (epochs_df['speech_type'] == 'Inner') & \n",
    "    (epochs_df['subject_number'] == 1)\n",
    "]\n",
    "\n",
    "print(f\"Found {len(inner_speech_sub1)} Inner speech epochs for subject 1\")\n",
    "print(\"Sample file paths:\")\n",
    "for filepath in inner_speech_sub1['relative_path'].head(3):\n",
    "    print(f\"  - {filepath}\")\n",
    "\n",
    "# Example 2: Load epochs by class\n",
    "print(\"\\n2. Loading all 'Up' class epochs:\")\n",
    "up_epochs = epochs_df[epochs_df['class'] == 'Up']\n",
    "print(f\"Found {len(up_epochs)} 'Up' class epochs\")\n",
    "print(f\"Distribution by speech type:\")\n",
    "print(up_epochs['speech_type'].value_counts())\n",
    "\n",
    "# Example 3: Create a function to load multiple epochs\n",
    "def load_epochs_by_criteria(metadata_df, **criteria):\n",
    "    \"\"\"\n",
    "    Load multiple epochs based on filtering criteria.\n",
    "    \n",
    "    Parameters:\n",
    "    - metadata_df: DataFrame with epoch metadata\n",
    "    - **criteria: Filtering criteria (e.g., subject_number=1, speech_type='Inner')\n",
    "    \n",
    "    Returns:\n",
    "    - List of loaded MNE Epochs objects\n",
    "    - Filtered metadata DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Apply filters\n",
    "    filtered_df = metadata_df.copy()\n",
    "    for key, value in criteria.items():\n",
    "        if isinstance(value, list):\n",
    "            filtered_df = filtered_df[filtered_df[key].isin(value)]\n",
    "        else:\n",
    "            filtered_df = filtered_df[filtered_df[key] == value]\n",
    "    \n",
    "    # Load epochs\n",
    "    loaded_epochs = []\n",
    "    for _, row in filtered_df.iterrows():\n",
    "        try:\n",
    "            epoch = mne.read_epochs(row['file_path'], verbose=False)\n",
    "            loaded_epochs.append(epoch)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load {row['filename']}: {e}\")\n",
    "    \n",
    "    return loaded_epochs, filtered_df\n",
    "\n",
    "print(\"\\n3. Example function usage:\")\n",
    "print(\"# Load all Inner speech 'Up' epochs from subjects 1-3\")\n",
    "print(\"epochs_list, metadata = load_epochs_by_criteria(\")\n",
    "print(\"    epochs_df,\")\n",
    "print(\"    subject_number=[1, 2, 3],\")\n",
    "print(\"    speech_type='Inner',\")\n",
    "print(\"    class='Up'\")\n",
    "print(\")\")\n",
    "\n",
    "# Test the function with a small sample\n",
    "sample_epochs, sample_metadata = load_epochs_by_criteria(\n",
    "    epochs_df.head(5),  # Just test with first 5 epochs\n",
    "    subject_number=1\n",
    ")\n",
    "\n",
    "print(f\"\\nTest function result: Loaded {len(sample_epochs)} epochs\")\n",
    "if len(sample_epochs) > 0:\n",
    "    print(f\"First epoch data shape: {sample_epochs[0].get_data().shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create Quick Access Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a quick access guide file\n",
    "guide_file = os.path.join(output_path, \"QUICK_ACCESS_GUIDE.md\")\n",
    "\n",
    "guide_content = f\"\"\"# Quick Access Guide - Individual MNE Epochs\n",
    "\n",
    "## Overview\n",
    "This dataset contains {len(epochs_df)} individual EEG epochs saved as MNE `.fif` files.\n",
    "\n",
    "## File Structure\n",
    "```\n",
    "{output_path}/\n",
    "├── individual_epochs/          # Individual MNE epoch files\n",
    "│   ├── sub-01/\n",
    "│   │   ├── ses-01/\n",
    "│   │   │   ├── sub-01_ses-01_trial_000_Inner_Up-epo.fif\n",
    "│   │   │   ├── sub-01_ses-01_trial_001_Inner_Down-epo.fif\n",
    "│   │   │   └── ...\n",
    "│   │   ├── ses-02/\n",
    "│   │   └── ses-03/\n",
    "│   ├── sub-02/\n",
    "│   └── ...\n",
    "├── metadata/                   # Metadata files\n",
    "│   ├── epochs_metadata.csv     # Human-readable metadata\n",
    "│   ├── epochs_metadata.pkl     # Python-optimized metadata\n",
    "│   └── extraction_summary.txt  # Summary statistics\n",
    "└── QUICK_ACCESS_GUIDE.md       # This guide\n",
    "```\n",
    "\n",
    "## Loading Individual Epochs\n",
    "\n",
    "### Method 1: Load Single Epoch\n",
    "```python\n",
    "import mne\n",
    "\n",
    "# Load a single epoch\n",
    "epoch = mne.read_epochs('individual_epochs/sub-01/ses-01/sub-01_ses-01_trial_000_Inner_Up-epo.fif')\n",
    "\n",
    "# Get data\n",
    "data = epoch.get_data()  # Shape: (1, n_channels, n_timepoints)\n",
    "metadata = epoch.metadata  # Pandas DataFrame with epoch info\n",
    "```\n",
    "\n",
    "### Method 2: Load Multiple Epochs Using Metadata\n",
    "```python\n",
    "import pandas as pd\n",
    "import mne\n",
    "\n",
    "# Load metadata\n",
    "metadata = pd.read_csv('metadata/epochs_metadata.csv')\n",
    "\n",
    "# Filter for specific criteria\n",
    "inner_speech_up = metadata[\n",
    "    (metadata['speech_type'] == 'Inner') & \n",
    "    (metadata['class'] == 'Up') &\n",
    "    (metadata['subject_number'] == 1)\n",
    "]\n",
    "\n",
    "# Load the epochs\n",
    "epochs_list = []\n",
    "for _, row in inner_speech_up.iterrows():\n",
    "    epoch = mne.read_epochs(row['file_path'])\n",
    "    epochs_list.append(epoch)\n",
    "```\n",
    "\n",
    "### Method 3: Combine Multiple Epochs\n",
    "```python\n",
    "# Load multiple epochs and combine them\n",
    "epochs_list = []  # Load as shown above\n",
    "\n",
    "# Combine into single Epochs object\n",
    "if len(epochs_list) > 1:\n",
    "    combined_epochs = mne.concatenate_epochs(epochs_list)\n",
    "else:\n",
    "    combined_epochs = epochs_list[0]\n",
    "\n",
    "# Now you can use all MNE functions\n",
    "combined_epochs.plot()  # Plot the epochs\n",
    "combined_epochs.average().plot()  # Plot average\n",
    "```\n",
    "\n",
    "## Metadata Columns\n",
    "- `epoch_id`: Unique identifier for each epoch\n",
    "- `subject_name`: Subject ID (sub-01, sub-02, ...)\n",
    "- `subject_number`: Subject number (1, 2, ...)\n",
    "- `session_number`: Session number (1, 2, 3)\n",
    "- `trial_number`: Trial number within session\n",
    "- `speech_type`: Type of speech (Pronounced, Inner, Visualized)\n",
    "- `class`: Direction class (Up, Down, Right, Left)\n",
    "- `timestamp`: Original timestamp from recording\n",
    "- `file_path`: Full path to the MNE file\n",
    "- `relative_path`: Relative path from output directory\n",
    "- `filename`: Just the filename\n",
    "\n",
    "## Common Use Cases\n",
    "\n",
    "### 1. Load all epochs for one subject\n",
    "```python\n",
    "subject_1_epochs = metadata[metadata['subject_number'] == 1]\n",
    "```\n",
    "\n",
    "### 2. Load specific speech type\n",
    "```python\n",
    "inner_speech = metadata[metadata['speech_type'] == 'Inner']\n",
    "```\n",
    "\n",
    "### 3. Load specific class for classification\n",
    "```python\n",
    "up_vs_down = metadata[metadata['class'].isin(['Up', 'Down'])]\n",
    "```\n",
    "\n",
    "### 4. Load by session\n",
    "```python\n",
    "session_1 = metadata[metadata['session_number'] == 1]\n",
    "```\n",
    "\n",
    "## Dataset Statistics\n",
    "- Total epochs: {len(epochs_df)}\n",
    "- Subjects: {epochs_df['subject_number'].nunique()}\n",
    "- Sessions per subject: {epochs_df.groupby('subject_number')['session_number'].nunique().iloc[0]}\n",
    "- Sampling frequency: {epochs_df['sampling_frequency'].iloc[0]} Hz\n",
    "- Channels: {epochs_df['n_channels'].iloc[0]}\n",
    "- Epoch duration: {epochs_df['duration_seconds'].iloc[0]:.2f} seconds\n",
    "\n",
    "## Speech Type Distribution\n",
    "\"\"\"\n",
    "\n",
    "for speech_type, count in speech_type_counts.items():\n",
    "    guide_content += f\"- {speech_type}: {count} epochs ({count/len(epochs_df)*100:.1f}%)\\n\"\n",
    "\n",
    "guide_content += \"\\n## Class Distribution\\n\"\n",
    "for class_name, count in class_counts.items():\n",
    "    guide_content += f\"- {class_name}: {count} epochs ({count/len(epochs_df)*100:.1f}%)\\n\"\n",
    "\n",
    "guide_content += \"\"\"\n",
    "## Tips\n",
    "1. Use the metadata CSV file to explore and filter epochs before loading\n",
    "2. Each epoch file contains exactly one trial with complete MNE metadata\n",
    "3. File names include subject, session, trial, speech type, and class for easy identification\n",
    "4. All standard MNE functions work with these epoch files\n",
    "5. Use `mne.concatenate_epochs()` to combine multiple epochs into one object\n",
    "\"\"\"\n",
    "\n",
    "with open(guide_file, 'w') as f:\n",
    "    f.write(guide_content)\n",
    "\n",
    "print(f\"Quick access guide saved to: {guide_file}\")\n",
    "print(f\"\\n=== ALL PROCESSING COMPLETE ===\")\n",
    "print(f\"\\nYour dataset is now organized as individual MNE epoch files!\")\n",
    "print(f\"Each epoch can be loaded directly with: mne.read_epochs('path/to/epoch-epo.fif')\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"1. Read the Quick Access Guide: {guide_file}\")\n",
    "print(f\"2. Explore the metadata: {metadata_file}\")\n",
    "print(f\"3. Start loading and analyzing individual epochs!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
