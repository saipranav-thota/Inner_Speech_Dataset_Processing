{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Extracted MNE Epochs\n",
    "\n",
    "This notebook processes all individual MNE epoch files to:\n",
    "- **Crop time window**: Keep only t = 1.0 to 3.5 seconds (2.5s duration)\n",
    "- **Bandpass filter**: Apply 70-100 Hz frequency filter (gamma band)\n",
    "- **Save processed files**: Create new optimized versions\n",
    "\n",
    "**Input**: `extracted_epochs_mne/` folder  \n",
    "**Output**: `processed_epochs/` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "# Configure MNE and warnings\n",
    "mne.set_log_level('WARNING')\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: D:\\VIT\\IV-Year\\PJT-I\\Speech Imagery Decoding\\Inner_Speech_Dataset\\Dataset\\extracted_epochs_mne\n",
      "Output: D:\\VIT\\IV-Year\\PJT-I\\Speech Imagery Decoding\\Inner_Speech_Dataset\\Dataset\\processed_epochs\n",
      "Time window: 1.0-3.5s (2.5s duration)\n",
      "Frequency: 13.0-70.0 Hz\n",
      "\n",
      "✓ Input directory found\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "INPUT_PATH = r\"D:\\VIT\\IV-Year\\PJT-I\\Speech Imagery Decoding\\Inner_Speech_Dataset\\Dataset\\extracted_epochs_mne\"\n",
    "OUTPUT_PATH = r\"D:\\VIT\\IV-Year\\PJT-I\\Speech Imagery Decoding\\Inner_Speech_Dataset\\Dataset\\processed_epochs\"\n",
    "\n",
    "# Processing parameters\n",
    "TIME_START = 1.0     # Start time (seconds)\n",
    "TIME_END = 3.5       # End time (seconds)\n",
    "FREQ_LOW = 13.0      # Low frequency (Hz)\n",
    "FREQ_HIGH = 70.0    # High frequency (Hz)\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_PATH}/individual_epochs\", exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_PATH}/metadata\", exist_ok=True)\n",
    "\n",
    "print(f\"Input: {INPUT_PATH}\")\n",
    "print(f\"Output: {OUTPUT_PATH}\")\n",
    "print(f\"Time window: {TIME_START}-{TIME_END}s ({TIME_END-TIME_START}s duration)\")\n",
    "print(f\"Frequency: {FREQ_LOW}-{FREQ_HIGH} Hz\")\n",
    "\n",
    "# Check input exists\n",
    "if not os.path.exists(INPUT_PATH):\n",
    "    print(f\"\\n❌ ERROR: {INPUT_PATH} not found!\")\n",
    "    print(\"Run the epoch extraction notebook first.\")\n",
    "else:\n",
    "    print(f\"\\n✓ Input directory found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded metadata for 5440 epochs\n",
      "Original duration: 4.50s\n",
      "Original time points: 1153\n",
      "Sampling frequency: 256.0 Hz\n"
     ]
    }
   ],
   "source": [
    "# Load original metadata\n",
    "metadata_file = f\"{INPUT_PATH}/metadata/epochs_metadata.pkl\"\n",
    "\n",
    "if os.path.exists(metadata_file):\n",
    "    original_metadata = pd.read_pickle(metadata_file)\n",
    "    print(f\"Loaded metadata for {len(original_metadata)} epochs\")\n",
    "    print(f\"Original duration: {original_metadata['duration_seconds'].iloc[0]:.2f}s\")\n",
    "    print(f\"Original time points: {original_metadata['n_timepoints'].iloc[0]}\")\n",
    "    print(f\"Sampling frequency: {original_metadata['sampling_frequency'].iloc[0]} Hz\")\n",
    "else:\n",
    "    print(f\"❌ ERROR: Metadata not found at {metadata_file}\")\n",
    "    raise FileNotFoundError(\"Run epoch extraction first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 5440 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 5440/5440 [18:20<00:00,  4.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PROCESSING COMPLETE ===\n",
      "✓ Successfully processed: 5440 epochs\n",
      "✗ Failed: 0 epochs\n",
      "Success rate: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def process_epoch(input_file, output_file):\n",
    "    \"\"\"Process a single epoch: filter and crop.\"\"\"\n",
    "    try:\n",
    "        # Load epoch\n",
    "        epoch = mne.read_epochs(input_file, verbose=False)\n",
    "        \n",
    "        # Apply bandpass filter (70-100 Hz)\n",
    "        epoch_filtered = epoch.copy().filter(\n",
    "            l_freq=FREQ_LOW, \n",
    "            h_freq=FREQ_HIGH, \n",
    "            fir_design='firwin',\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Crop time window (1.0-3.5s)\n",
    "        epoch_processed = epoch_filtered.crop(\n",
    "            tmin=TIME_START, \n",
    "            tmax=TIME_END,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "        \n",
    "        # Save processed epoch\n",
    "        epoch_processed.save(output_file, overwrite=True, verbose=False)\n",
    "        \n",
    "        # Return new metadata\n",
    "        return {\n",
    "            'success': True,\n",
    "            'duration_seconds': TIME_END - TIME_START,\n",
    "            'n_timepoints': epoch_processed.get_data().shape[2],\n",
    "            'time_start': TIME_START,\n",
    "            'time_end': TIME_END,\n",
    "            'freq_low': FREQ_LOW,\n",
    "            'freq_high': FREQ_HIGH,\n",
    "            'processed': True\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {input_file}: {e}\")\n",
    "        return {'success': False}\n",
    "\n",
    "# Process all epochs\n",
    "print(f\"\\nProcessing {len(original_metadata)} epochs...\")\n",
    "\n",
    "processed_metadata = []\n",
    "success_count = 0\n",
    "fail_count = 0\n",
    "\n",
    "for idx, row in tqdm(original_metadata.iterrows(), total=len(original_metadata), desc=\"Processing\"):\n",
    "    # Rebuild paths properly\n",
    "    input_file = os.path.join(INPUT_PATH, row['relative_path'])\n",
    "    output_file = os.path.join(OUTPUT_PATH, row['relative_path'])\n",
    "        \n",
    "    # Process epoch\n",
    "    result = process_epoch(input_file, output_file)\n",
    "    \n",
    "    if result['success']:\n",
    "        # Update metadata - convert Series to dict first to avoid KeyError\n",
    "        updated_row = row.to_dict()\n",
    "        updated_row.update(result)\n",
    "        updated_row['file_path'] = output_file\n",
    "        updated_row['relative_path'] = os.path.relpath(output_file, OUTPUT_PATH)\n",
    "        \n",
    "        processed_metadata.append(updated_row)\n",
    "        success_count += 1\n",
    "    else:\n",
    "        fail_count += 1\n",
    "\n",
    "processed_df = pd.DataFrame(processed_metadata)\n",
    "\n",
    "print(f\"\\n=== PROCESSING COMPLETE ===\")\n",
    "print(f\"✓ Successfully processed: {success_count} epochs\")\n",
    "print(f\"✗ Failed: {fail_count} epochs\")\n",
    "print(f\"Success rate: {success_count/(success_count+fail_count)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PROCESSING RESULTS ===\n",
      "Processed epochs: 5440\n",
      "New duration: 2.50s\n",
      "New time points: 641\n",
      "Time window: 1.0-3.5s\n",
      "Frequency: 13.0-70.0 Hz\n",
      "\n",
      "=== BEFORE vs AFTER ===\n",
      "Duration: 4.50s → 2.50s\n",
      "Time points: 1153 → 641\n",
      "Data reduction: 44.4%\n",
      "\n",
      "=== DISTRIBUTION CHECK ===\n",
      "Speech types: {'Visualized': 2196, 'Inner': 2156, 'Pronounced': 1088}\n",
      "Classes: {'Left': 1360, 'Up': 1360, 'Right': 1360, 'Down': 1360}\n",
      "Subjects: 10\n"
     ]
    }
   ],
   "source": [
    "# Analyze results\n",
    "if len(processed_df) > 0:\n",
    "    print(\"=== PROCESSING RESULTS ===\")\n",
    "    print(f\"Processed epochs: {len(processed_df)}\")\n",
    "    print(f\"New duration: {processed_df['duration_seconds'].iloc[0]:.2f}s\")\n",
    "    print(f\"New time points: {processed_df['n_timepoints'].iloc[0]}\")\n",
    "    print(f\"Time window: {processed_df['time_start'].iloc[0]}-{processed_df['time_end'].iloc[0]}s\")\n",
    "    print(f\"Frequency: {processed_df['freq_low'].iloc[0]}-{processed_df['freq_high'].iloc[0]} Hz\")\n",
    "    \n",
    "    print(\"\\n=== BEFORE vs AFTER ===\")\n",
    "    orig_duration = original_metadata['duration_seconds'].iloc[0]\n",
    "    new_duration = processed_df['duration_seconds'].iloc[0]\n",
    "    orig_points = original_metadata['n_timepoints'].iloc[0]\n",
    "    new_points = processed_df['n_timepoints'].iloc[0]\n",
    "    \n",
    "    print(f\"Duration: {orig_duration:.2f}s → {new_duration:.2f}s\")\n",
    "    print(f\"Time points: {orig_points} → {new_points}\")\n",
    "    print(f\"Data reduction: {(1 - new_points/orig_points)*100:.1f}%\")\n",
    "    \n",
    "    print(\"\\n=== DISTRIBUTION CHECK ===\")\n",
    "    print(\"Speech types:\", processed_df['speech_type'].value_counts().to_dict())\n",
    "    print(\"Classes:\", processed_df['class'].value_counts().to_dict())\n",
    "    print(\"Subjects:\", processed_df['subject_number'].nunique())\n",
    "else:\n",
    "    print(\"❌ No epochs were successfully processed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TESTING PROCESSED EPOCHS ===\n",
      "\n",
      "Test 1: sub-01_ses-01_trial_000\n",
      "  ✓ Loaded successfully\n",
      "  Data shape: (1, 128, 641)\n",
      "  Time: 1.00 to 3.50s\n",
      "  Duration: 2.50s\n",
      "  ✓ Duration verified: 2.50s\n",
      "\n",
      "Test 2: sub-01_ses-01_trial_001\n",
      "  ✓ Loaded successfully\n",
      "  Data shape: (1, 128, 641)\n",
      "  Time: 1.00 to 3.50s\n",
      "  Duration: 2.50s\n",
      "  ✓ Duration verified: 2.50s\n",
      "\n",
      "Test 3: sub-01_ses-01_trial_002\n",
      "  ✓ Loaded successfully\n",
      "  Data shape: (1, 128, 641)\n",
      "  Time: 1.00 to 3.50s\n",
      "  Duration: 2.50s\n",
      "  ✓ Duration verified: 2.50s\n"
     ]
    }
   ],
   "source": [
    "# Test loading processed epochs\n",
    "if len(processed_df) > 0:\n",
    "    print(\"=== TESTING PROCESSED EPOCHS ===\")\n",
    "    \n",
    "    # Test first 3 epochs\n",
    "    for i, (_, row) in enumerate(processed_df.head(3).iterrows()):\n",
    "        print(f\"\\nTest {i+1}: {row['epoch_id']}\")\n",
    "        \n",
    "        try:\n",
    "            epoch = mne.read_epochs(row['file_path'], verbose=False)\n",
    "            data = epoch.get_data()\n",
    "            \n",
    "            print(f\"  ✓ Loaded successfully\")\n",
    "            print(f\"  Data shape: {data.shape}\")\n",
    "            print(f\"  Time: {epoch.tmin:.2f} to {epoch.tmax:.2f}s\")\n",
    "            print(f\"  Duration: {epoch.tmax - epoch.tmin:.2f}s\")\n",
    "            \n",
    "            # Verify duration\n",
    "            expected = TIME_END - TIME_START\n",
    "            actual = epoch.tmax - epoch.tmin\n",
    "            if abs(actual - expected) < 0.01:\n",
    "                print(f\"  ✓ Duration verified: {actual:.2f}s\")\n",
    "            else:\n",
    "                print(f\"  ⚠ Duration issue: {actual:.2f}s (expected {expected:.2f}s)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "else:\n",
    "    print(\"No processed epochs to test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FILES SAVED ===\n",
      "✓ 5440 processed epoch files\n",
      "✓ Metadata: D:\\VIT\\IV-Year\\PJT-I\\Speech Imagery Decoding\\Inner_Speech_Dataset\\Dataset\\processed_epochs/metadata/processed_epochs_metadata.csv\n",
      "✓ Metadata: D:\\VIT\\IV-Year\\PJT-I\\Speech Imagery Decoding\\Inner_Speech_Dataset\\Dataset\\processed_epochs/metadata/processed_epochs_metadata.pkl\n",
      "✓ Summary: D:\\VIT\\IV-Year\\PJT-I\\Speech Imagery Decoding\\Inner_Speech_Dataset\\Dataset\\processed_epochs/metadata/processing_summary.txt\n",
      "\n",
      "=== USAGE ===\n",
      "Load single epoch:\n",
      "  epoch = mne.read_epochs('processed_epochs/individual_epochs/...')\n",
      "\n",
      "Load metadata:\n",
      "  metadata = pd.read_csv('D:\\VIT\\IV-Year\\PJT-I\\Speech Imagery Decoding\\Inner_Speech_Dataset\\Dataset\\processed_epochs/metadata/processed_epochs_metadata.csv')\n",
      "\n",
      "Filter epochs:\n",
      "  inner_speech = metadata[metadata['speech_type'] == 'Inner']\n",
      "\n",
      "🎉 PROCESSING COMPLETE!\n",
      "Your epochs are now:\n",
      "  • Cropped to 1.0-3.5s (2.5s duration)\n",
      "  • Filtered to 13.0-70.0 Hz (gamma band)\n",
      "  • 44.4% smaller in size\n",
      "  • Ready for analysis in: D:\\VIT\\IV-Year\\PJT-I\\Speech Imagery Decoding\\Inner_Speech_Dataset\\Dataset\\processed_epochs\n"
     ]
    }
   ],
   "source": [
    "# Save processed metadata and summary\n",
    "if len(processed_df) > 0:\n",
    "    # Save metadata files\n",
    "    csv_file = f\"{OUTPUT_PATH}/metadata/processed_epochs_metadata.csv\"\n",
    "    pkl_file = f\"{OUTPUT_PATH}/metadata/processed_epochs_metadata.pkl\"\n",
    "    summary_file = f\"{OUTPUT_PATH}/metadata/processing_summary.txt\"\n",
    "    \n",
    "    processed_df.to_csv(csv_file, index=False)\n",
    "    processed_df.to_pickle(pkl_file)\n",
    "    \n",
    "    # Create summary\n",
    "    with open(summary_file, 'w') as f:\n",
    "        f.write(\"PROCESSED EPOCHS SUMMARY\\n\")\n",
    "        f.write(\"=\" * 40 + \"\\n\\n\")\n",
    "        f.write(f\"Processing Parameters:\\n\")\n",
    "        f.write(f\"  Time window: {TIME_START} - {TIME_END} seconds\\n\")\n",
    "        f.write(f\"  Duration: {TIME_END - TIME_START} seconds\\n\")\n",
    "        f.write(f\"  Frequency filter: {FREQ_LOW} - {FREQ_HIGH} Hz\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Results:\\n\")\n",
    "        f.write(f\"  Successfully processed: {success_count} epochs\\n\")\n",
    "        f.write(f\"  Failed: {fail_count} epochs\\n\")\n",
    "        f.write(f\"  Success rate: {success_count/(success_count+fail_count)*100:.1f}%\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Data Changes:\\n\")\n",
    "        f.write(f\"  Original duration: {original_metadata['duration_seconds'].iloc[0]:.2f}s\\n\")\n",
    "        f.write(f\"  New duration: {processed_df['duration_seconds'].iloc[0]:.2f}s\\n\")\n",
    "        f.write(f\"  Original time points: {original_metadata['n_timepoints'].iloc[0]}\\n\")\n",
    "        f.write(f\"  New time points: {processed_df['n_timepoints'].iloc[0]}\\n\")\n",
    "        f.write(f\"  Data reduction: {(1 - processed_df['n_timepoints'].iloc[0]/original_metadata['n_timepoints'].iloc[0])*100:.1f}%\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Dataset Info:\\n\")\n",
    "        f.write(f\"  Subjects: {processed_df['subject_number'].nunique()}\\n\")\n",
    "        f.write(f\"  Sampling frequency: {processed_df['sampling_frequency'].iloc[0]} Hz\\n\")\n",
    "        f.write(f\"  Channels: {processed_df['n_channels'].iloc[0]}\\n\")\n",
    "    \n",
    "    print(f\"\\n=== FILES SAVED ===\")\n",
    "    print(f\"✓ {len(processed_df)} processed epoch files\")\n",
    "    print(f\"✓ Metadata: {csv_file}\")\n",
    "    print(f\"✓ Metadata: {pkl_file}\")\n",
    "    print(f\"✓ Summary: {summary_file}\")\n",
    "    \n",
    "    print(f\"\\n=== USAGE ===\")\n",
    "    print(f\"Load single epoch:\")\n",
    "    print(f\"  epoch = mne.read_epochs('processed_epochs/individual_epochs/...')\")\n",
    "    print(f\"\\nLoad metadata:\")\n",
    "    print(f\"  metadata = pd.read_csv('{csv_file}')\")\n",
    "    print(f\"\\nFilter epochs:\")\n",
    "    print(f\"  inner_speech = metadata[metadata['speech_type'] == 'Inner']\")\n",
    "    \n",
    "    print(f\"\\n🎉 PROCESSING COMPLETE!\")\n",
    "    print(f\"Your epochs are now:\")\n",
    "    print(f\"  • Cropped to {TIME_START}-{TIME_END}s ({TIME_END-TIME_START}s duration)\")\n",
    "    print(f\"  • Filtered to {FREQ_LOW}-{FREQ_HIGH} Hz (gamma band)\")\n",
    "    print(f\"  • {(1 - processed_df['n_timepoints'].iloc[0]/original_metadata['n_timepoints'].iloc[0])*100:.1f}% smaller in size\")\n",
    "    print(f\"  • Ready for analysis in: {OUTPUT_PATH}\")\n",
    "else:\n",
    "    print(\"❌ No processed data to save\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
