{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Z-Score Normalization for EEG Datasets\n",
    "\n",
    "This notebook applies z-score normalization to EEG datasets. It can handle:\n",
    "- **Individual channel files** (.npy format)\n",
    "- **MNE epoch files** (.fif format)\n",
    "- **Mixed datasets** (automatically detects file types)\n",
    "\n",
    "## Z-Score Formula:\n",
    "```\n",
    "z = (x - μ) / σ\n",
    "```\n",
    "Where:\n",
    "- `x` = original value\n",
    "- `μ` = mean\n",
    "- `σ` = standard deviation\n",
    "\n",
    "## Normalization Options:\n",
    "1. **Per-file normalization**: Each file normalized independently\n",
    "2. **Global normalization**: All files normalized using global statistics\n",
    "3. **Per-channel normalization**: Each channel normalized independently (for multi-channel data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import mne\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "# Configure settings\n",
    "mne.set_log_level('WARNING')\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Input directory: D:\\VIT\\IV-Year\\PJT-I\\Speech Imagery Decoding\\Inner_Speech_Dataset\\Dataset\\individual_channels/by_epoch\n",
      "  Output directory: D:\\VIT\\IV-Year\\PJT-I\\Speech Imagery Decoding\\Inner_Speech_Dataset\\Dataset\\normalized_data\n",
      "  Normalization type: per_file\n",
      "  Preserve structure: True\n",
      "  Backup original: False\n",
      "\n",
      "✓ Input directory found\n",
      "✓ Output directory created\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "INPUT_DIR = r\"D:\\VIT\\IV-Year\\PJT-I\\Speech Imagery Decoding\\Inner_Speech_Dataset\\Dataset\\individual_channels/by_epoch\"\n",
    "OUTPUT_DIR = r\"D:\\VIT\\IV-Year\\PJT-I\\Speech Imagery Decoding\\Inner_Speech_Dataset\\Dataset\\normalized_data\"\n",
    "\n",
    "# Normalization options\n",
    "NORMALIZATION_TYPE = \"per_file\"    # Options: \"per_file\", \"global\", \"per_channel\"\n",
    "PRESERVE_STRUCTURE = True          # Keep original folder structure\n",
    "BACKUP_ORIGINAL = False            # Create backup of original data\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Input directory: {INPUT_DIR}\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"  Normalization type: {NORMALIZATION_TYPE}\")\n",
    "print(f\"  Preserve structure: {PRESERVE_STRUCTURE}\")\n",
    "print(f\"  Backup original: {BACKUP_ORIGINAL}\")\n",
    "\n",
    "# Check input directory\n",
    "if not os.path.exists(INPUT_DIR):\n",
    "    print(f\"\\n❌ ERROR: Input directory '{INPUT_DIR}' not found!\")\n",
    "    print(\"Please specify a valid dataset directory.\")\n",
    "else:\n",
    "    print(f\"\\n✓ Input directory found\")\n",
    "    \n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_DIR}/metadata\", exist_ok=True)\n",
    "print(f\"✓ Output directory created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing input directory...\n",
      "\n",
      "=== FILE ANALYSIS ===\n",
      "Total files: 696320\n",
      "NumPy files (.npy): 696320\n",
      "MNE files (.fif): 0\n",
      "Other files: 0\n",
      "\n",
      "Sample .npy files:\n",
      "  1. sub-01_ses-01_trial_000\\sub-01_ses-01_trial_000_Pronounced_Left_A1.npy\n",
      "  2. sub-01_ses-01_trial_000\\sub-01_ses-01_trial_000_Pronounced_Left_A10.npy\n",
      "  3. sub-01_ses-01_trial_000\\sub-01_ses-01_trial_000_Pronounced_Left_A11.npy\n",
      "\n",
      "Primary file type: npy\n",
      "Files to process: 696320\n"
     ]
    }
   ],
   "source": [
    "def detect_file_types(directory):\n",
    "    \"\"\"\n",
    "    Detect what types of files are in the directory.\n",
    "    \n",
    "    Returns:\n",
    "    - file_info: Dictionary with file type statistics\n",
    "    \"\"\"\n",
    "    file_info = {\n",
    "        'npy_files': [],\n",
    "        'fif_files': [],\n",
    "        'other_files': [],\n",
    "        'total_files': 0\n",
    "    }\n",
    "    \n",
    "    # Walk through directory\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            file_info['total_files'] += 1\n",
    "            \n",
    "            if file.endswith('.npy'):\n",
    "                file_info['npy_files'].append(file_path)\n",
    "            elif file.endswith('.fif'):\n",
    "                file_info['fif_files'].append(file_path)\n",
    "            else:\n",
    "                file_info['other_files'].append(file_path)\n",
    "    \n",
    "    return file_info\n",
    "\n",
    "# Analyze input directory\n",
    "print(\"Analyzing input directory...\")\n",
    "file_info = detect_file_types(INPUT_DIR)\n",
    "\n",
    "print(f\"\\n=== FILE ANALYSIS ===\")\n",
    "print(f\"Total files: {file_info['total_files']}\")\n",
    "print(f\"NumPy files (.npy): {len(file_info['npy_files'])}\")\n",
    "print(f\"MNE files (.fif): {len(file_info['fif_files'])}\")\n",
    "print(f\"Other files: {len(file_info['other_files'])}\")\n",
    "\n",
    "if len(file_info['npy_files']) > 0:\n",
    "    print(f\"\\nSample .npy files:\")\n",
    "    for i, file in enumerate(file_info['npy_files'][:3]):\n",
    "        print(f\"  {i+1}. {os.path.relpath(file, INPUT_DIR)}\")\n",
    "\n",
    "if len(file_info['fif_files']) > 0:\n",
    "    print(f\"\\nSample .fif files:\")\n",
    "    for i, file in enumerate(file_info['fif_files'][:3]):\n",
    "        print(f\"  {i+1}. {os.path.relpath(file, INPUT_DIR)}\")\n",
    "\n",
    "# Determine primary file type\n",
    "if len(file_info['npy_files']) > len(file_info['fif_files']):\n",
    "    primary_type = 'npy'\n",
    "    primary_files = file_info['npy_files']\n",
    "elif len(file_info['fif_files']) > 0:\n",
    "    primary_type = 'fif'\n",
    "    primary_files = file_info['fif_files']\n",
    "else:\n",
    "    primary_type = None\n",
    "    primary_files = []\n",
    "\n",
    "print(f\"\\nPrimary file type: {primary_type}\")\n",
    "print(f\"Files to process: {len(primary_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TESTING FILE LOADING ===\n",
      "Testing: sub-01_ses-01_trial_000\\sub-01_ses-01_trial_000_Pronounced_Left_A1.npy\n",
      "✓ Successfully loaded\n",
      "  Shape: (641,)\n",
      "  Data type: float64\n",
      "  Data range: -2.17e-05 to 1.40e-05\n",
      "  Mean: -1.08e-08\n",
      "  Std: 5.64e-06\n",
      "  Metadata: {'file_type': 'npy', 'original_shape': (641,), 'original_dtype': 'float64'}\n"
     ]
    }
   ],
   "source": [
    "def load_data_file(file_path, file_type):\n",
    "    \"\"\"\n",
    "    Load data from file based on type.\n",
    "    \n",
    "    Returns:\n",
    "    - data: numpy array\n",
    "    - metadata: dict with file info\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if file_type == 'npy':\n",
    "            data = np.load(file_path)\n",
    "            metadata = {\n",
    "                'file_type': 'npy',\n",
    "                'original_shape': data.shape,\n",
    "                'original_dtype': str(data.dtype)\n",
    "            }\n",
    "            \n",
    "        elif file_type == 'fif':\n",
    "            epoch = mne.read_epochs(file_path, verbose=False)\n",
    "            data = epoch.get_data()\n",
    "            metadata = {\n",
    "                'file_type': 'fif',\n",
    "                'original_shape': data.shape,\n",
    "                'original_dtype': str(data.dtype),\n",
    "                'sampling_freq': epoch.info['sfreq'],\n",
    "                'n_channels': len(epoch.ch_names),\n",
    "                'channel_names': epoch.ch_names\n",
    "            }\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type: {file_type}\")\n",
    "            \n",
    "        return data, metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def save_normalized_data(data, output_path, file_type, original_metadata=None):\n",
    "    \"\"\"\n",
    "    Save normalized data in the same format as original.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create output directory\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        \n",
    "        if file_type == 'npy':\n",
    "            np.save(output_path, data)\n",
    "            \n",
    "        elif file_type == 'fif':\n",
    "            # For .fif files, we need to reconstruct the MNE object\n",
    "            # This is more complex and requires original epoch structure\n",
    "            # For now, save as .npy and note the conversion\n",
    "            output_path = output_path.replace('.fif', '_normalized.npy')\n",
    "            np.save(output_path, data)\n",
    "            \n",
    "        return True, output_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {output_path}: {e}\")\n",
    "        return False, None\n",
    "\n",
    "# Test loading a sample file\n",
    "if len(primary_files) > 0:\n",
    "    print(f\"\\n=== TESTING FILE LOADING ===\")\n",
    "    sample_file = primary_files[0]\n",
    "    print(f\"Testing: {os.path.relpath(sample_file, INPUT_DIR)}\")\n",
    "    \n",
    "    data, metadata = load_data_file(sample_file, primary_type)\n",
    "    \n",
    "    if data is not None:\n",
    "        print(f\"✓ Successfully loaded\")\n",
    "        print(f\"  Shape: {data.shape}\")\n",
    "        print(f\"  Data type: {data.dtype}\")\n",
    "        print(f\"  Data range: {data.min():.2e} to {data.max():.2e}\")\n",
    "        print(f\"  Mean: {data.mean():.2e}\")\n",
    "        print(f\"  Std: {data.std():.2e}\")\n",
    "        \n",
    "        if metadata:\n",
    "            print(f\"  Metadata: {metadata}\")\n",
    "    else:\n",
    "        print(f\"✗ Failed to load sample file\")\n",
    "else:\n",
    "    print(f\"\\n❌ No processable files found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ready to normalize 696320 files using per_file normalization\n"
     ]
    }
   ],
   "source": [
    "def compute_global_statistics(file_list, file_type):\n",
    "    \"\"\"\n",
    "    Compute global mean and std across all files for global normalization.\n",
    "    \"\"\"\n",
    "    print(\"Computing global statistics...\")\n",
    "    \n",
    "    all_values = []\n",
    "    n_samples = 0\n",
    "    \n",
    "    # Sample a subset of files for efficiency if dataset is very large\n",
    "    sample_size = min(len(file_list), 1000)  # Sample up to 1000 files\n",
    "    sample_files = np.random.choice(file_list, sample_size, replace=False)\n",
    "    \n",
    "    for file_path in tqdm(sample_files, desc=\"Computing global stats\"):\n",
    "        data, _ = load_data_file(file_path, file_type)\n",
    "        if data is not None:\n",
    "            all_values.extend(data.flatten())\n",
    "            n_samples += data.size\n",
    "            \n",
    "            # Limit memory usage\n",
    "            if len(all_values) > 10_000_000:  # 10M samples\n",
    "                break\n",
    "    \n",
    "    if len(all_values) > 0:\n",
    "        global_mean = np.mean(all_values)\n",
    "        global_std = np.std(all_values)\n",
    "        \n",
    "        print(f\"Global statistics computed from {len(sample_files)} files:\")\n",
    "        print(f\"  Samples analyzed: {len(all_values):,}\")\n",
    "        print(f\"  Global mean: {global_mean:.6e}\")\n",
    "        print(f\"  Global std: {global_std:.6e}\")\n",
    "        \n",
    "        return global_mean, global_std\n",
    "    else:\n",
    "        print(\"❌ Could not compute global statistics\")\n",
    "        return None, None\n",
    "\n",
    "def normalize_data(data, normalization_type, global_mean=None, global_std=None):\n",
    "    \"\"\"\n",
    "    Apply z-score normalization to data.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: numpy array\n",
    "    - normalization_type: \"per_file\", \"global\", or \"per_channel\"\n",
    "    - global_mean, global_std: for global normalization\n",
    "    \n",
    "    Returns:\n",
    "    - normalized_data: z-score normalized data\n",
    "    - norm_stats: normalization statistics used\n",
    "    \"\"\"\n",
    "    \n",
    "    if normalization_type == \"per_file\":\n",
    "        # Normalize entire file using file's mean and std\n",
    "        mean = np.mean(data)\n",
    "        std = np.std(data)\n",
    "        \n",
    "        if std == 0:\n",
    "            normalized_data = np.zeros_like(data)\n",
    "        else:\n",
    "            normalized_data = (data - mean) / std\n",
    "            \n",
    "        norm_stats = {'mean': mean, 'std': std, 'type': 'per_file'}\n",
    "        \n",
    "    elif normalization_type == \"global\":\n",
    "        # Use global statistics\n",
    "        if global_mean is None or global_std is None:\n",
    "            raise ValueError(\"Global statistics not provided\")\n",
    "            \n",
    "        if global_std == 0:\n",
    "            normalized_data = np.zeros_like(data)\n",
    "        else:\n",
    "            normalized_data = (data - global_mean) / global_std\n",
    "            \n",
    "        norm_stats = {'mean': global_mean, 'std': global_std, 'type': 'global'}\n",
    "        \n",
    "    elif normalization_type == \"per_channel\":\n",
    "        # Normalize each channel independently\n",
    "        normalized_data = np.zeros_like(data)\n",
    "        norm_stats = {'type': 'per_channel', 'channel_stats': []}\n",
    "        \n",
    "        if data.ndim == 1:\n",
    "            # 1D data - treat as single channel\n",
    "            mean = np.mean(data)\n",
    "            std = np.std(data)\n",
    "            if std == 0:\n",
    "                normalized_data = np.zeros_like(data)\n",
    "            else:\n",
    "                normalized_data = (data - mean) / std\n",
    "            norm_stats['channel_stats'].append({'mean': mean, 'std': std})\n",
    "            \n",
    "        elif data.ndim == 2:\n",
    "            # 2D data - normalize each row (channel)\n",
    "            for ch in range(data.shape[0]):\n",
    "                mean = np.mean(data[ch])\n",
    "                std = np.std(data[ch])\n",
    "                if std == 0:\n",
    "                    normalized_data[ch] = np.zeros_like(data[ch])\n",
    "                else:\n",
    "                    normalized_data[ch] = (data[ch] - mean) / std\n",
    "                norm_stats['channel_stats'].append({'mean': mean, 'std': std})\n",
    "                \n",
    "        elif data.ndim == 3:\n",
    "            # 3D data (epochs, channels, time) - normalize each channel across time\n",
    "            for ep in range(data.shape[0]):\n",
    "                for ch in range(data.shape[1]):\n",
    "                    mean = np.mean(data[ep, ch])\n",
    "                    std = np.std(data[ep, ch])\n",
    "                    if std == 0:\n",
    "                        normalized_data[ep, ch] = np.zeros_like(data[ep, ch])\n",
    "                    else:\n",
    "                        normalized_data[ep, ch] = (data[ep, ch] - mean) / std\n",
    "                    norm_stats['channel_stats'].append({'epoch': ep, 'channel': ch, 'mean': mean, 'std': std})\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown normalization type: {normalization_type}\")\n",
    "    \n",
    "    return normalized_data, norm_stats\n",
    "\n",
    "# Compute global statistics if needed\n",
    "global_mean, global_std = None, None\n",
    "\n",
    "if NORMALIZATION_TYPE == \"global\" and len(primary_files) > 0:\n",
    "    global_mean, global_std = compute_global_statistics(primary_files, primary_type)\n",
    "    \n",
    "    if global_mean is None:\n",
    "        print(\"❌ Failed to compute global statistics. Switching to per-file normalization.\")\n",
    "        NORMALIZATION_TYPE = \"per_file\"\n",
    "\n",
    "print(f\"\\nReady to normalize {len(primary_files)} files using {NORMALIZATION_TYPE} normalization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== NORMALIZING 696320 FILES ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing (per_file): 100%|██████████| 696320/696320 [34:42<00:00, 334.41it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== NORMALIZATION COMPLETE ===\n",
      "✓ Successfully normalized: 696320 files\n",
      "✗ Failed: 0 files\n",
      "Success rate: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process all files\n",
    "if len(primary_files) > 0:\n",
    "    print(f\"\\n=== NORMALIZING {len(primary_files)} FILES ===\")\n",
    "    \n",
    "    normalization_results = []\n",
    "    success_count = 0\n",
    "    fail_count = 0\n",
    "    \n",
    "    for file_path in tqdm(primary_files, desc=f\"Normalizing ({NORMALIZATION_TYPE})\"):\n",
    "        try:\n",
    "            # Load original data\n",
    "            data, metadata = load_data_file(file_path, primary_type)\n",
    "            \n",
    "            if data is None:\n",
    "                fail_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Apply normalization\n",
    "            normalized_data, norm_stats = normalize_data(\n",
    "                data, NORMALIZATION_TYPE, global_mean, global_std\n",
    "            )\n",
    "            \n",
    "            # Determine output path\n",
    "            if PRESERVE_STRUCTURE:\n",
    "                rel_path = os.path.relpath(file_path, INPUT_DIR)\n",
    "                output_path = os.path.join(OUTPUT_DIR, rel_path)\n",
    "            else:\n",
    "                filename = os.path.basename(file_path)\n",
    "                output_path = os.path.join(OUTPUT_DIR, filename)\n",
    "            \n",
    "            # Save normalized data\n",
    "            save_success, final_output_path = save_normalized_data(\n",
    "                normalized_data, output_path, primary_type, metadata\n",
    "            )\n",
    "            \n",
    "            if save_success:\n",
    "                # Record results\n",
    "                result = {\n",
    "                    'original_file': file_path,\n",
    "                    'normalized_file': final_output_path,\n",
    "                    'original_shape': data.shape,\n",
    "                    'original_mean': float(np.mean(data)),\n",
    "                    'original_std': float(np.std(data)),\n",
    "                    'original_min': float(np.min(data)),\n",
    "                    'original_max': float(np.max(data)),\n",
    "                    'normalized_mean': float(np.mean(normalized_data)),\n",
    "                    'normalized_std': float(np.std(normalized_data)),\n",
    "                    'normalized_min': float(np.min(normalized_data)),\n",
    "                    'normalized_max': float(np.max(normalized_data)),\n",
    "                    'normalization_type': NORMALIZATION_TYPE,\n",
    "                    'file_type': primary_type\n",
    "                }\n",
    "                \n",
    "                # Add normalization stats\n",
    "                if NORMALIZATION_TYPE in ['per_file', 'global']:\n",
    "                    result['norm_mean'] = float(norm_stats['mean'])\n",
    "                    result['norm_std'] = float(norm_stats['std'])\n",
    "                \n",
    "                normalization_results.append(result)\n",
    "                success_count += 1\n",
    "            else:\n",
    "                fail_count += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "            fail_count += 1\n",
    "    \n",
    "    print(f\"\\n=== NORMALIZATION COMPLETE ===\")\n",
    "    print(f\"✓ Successfully normalized: {success_count} files\")\n",
    "    print(f\"✗ Failed: {fail_count} files\")\n",
    "    print(f\"Success rate: {success_count/(success_count+fail_count)*100:.1f}%\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No files to process\")\n",
    "    normalization_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NORMALIZATION ANALYSIS ===\n",
      "Files processed: 696320\n",
      "Normalization type: per_file\n",
      "\n",
      "=== BEFORE NORMALIZATION ===\n",
      "Mean range: -8.60e-07 to 3.08e-07\n",
      "Std range: 9.63e-07 to 6.05e-05\n",
      "Data range: -6.03e-04 to 5.37e-04\n",
      "Average mean: -4.81e-10\n",
      "Average std: 4.66e-06\n",
      "\n",
      "=== AFTER NORMALIZATION ===\n",
      "Mean range: -6.03e-17 to 5.68e-17\n",
      "Std range: 1.00e+00 to 1.00e+00\n",
      "Data range: -1.43e+01 to 1.41e+01\n",
      "Average mean: 3.56e-20\n",
      "Average std: 1.00e+00\n",
      "\n",
      "=== NORMALIZATION QUALITY CHECK ===\n",
      "Files with mean ≈ 0: 696320/696320 (100.0%)\n",
      "Files with std ≈ 1: 696320/696320 (100.0%)\n",
      "✓ Normalization quality: EXCELLENT\n",
      "\n",
      "=== SAMPLE RESULTS ===\n",
      "\n",
      "File 1: sub-01_ses-01_trial_000_Pronounced_Left_A1.npy\n",
      "  Original: mean=-1.08e-08, std=5.64e-06\n",
      "  Normalized: mean=-1.66e-17, std=1.00e+00\n",
      "  Output: sub-01_ses-01_trial_000\\sub-01_ses-01_trial_000_Pronounced_Left_A1.npy\n",
      "\n",
      "File 2: sub-01_ses-01_trial_000_Pronounced_Left_A10.npy\n",
      "  Original: mean=-1.42e-08, std=6.97e-06\n",
      "  Normalized: mean=1.32e-17, std=1.00e+00\n",
      "  Output: sub-01_ses-01_trial_000\\sub-01_ses-01_trial_000_Pronounced_Left_A10.npy\n",
      "\n",
      "File 3: sub-01_ses-01_trial_000_Pronounced_Left_A11.npy\n",
      "  Original: mean=-2.22e-09, std=7.79e-06\n",
      "  Normalized: mean=2.88e-17, std=1.00e+00\n",
      "  Output: sub-01_ses-01_trial_000\\sub-01_ses-01_trial_000_Pronounced_Left_A11.npy\n"
     ]
    }
   ],
   "source": [
    "# Analyze normalization results\n",
    "if len(normalization_results) > 0:\n",
    "    results_df = pd.DataFrame(normalization_results)\n",
    "    \n",
    "    print(\"=== NORMALIZATION ANALYSIS ===\")\n",
    "    print(f\"Files processed: {len(results_df)}\")\n",
    "    print(f\"Normalization type: {NORMALIZATION_TYPE}\")\n",
    "    \n",
    "    print(\"\\n=== BEFORE NORMALIZATION ===\")\n",
    "    print(f\"Mean range: {results_df['original_mean'].min():.2e} to {results_df['original_mean'].max():.2e}\")\n",
    "    print(f\"Std range: {results_df['original_std'].min():.2e} to {results_df['original_std'].max():.2e}\")\n",
    "    print(f\"Data range: {results_df['original_min'].min():.2e} to {results_df['original_max'].max():.2e}\")\n",
    "    print(f\"Average mean: {results_df['original_mean'].mean():.2e}\")\n",
    "    print(f\"Average std: {results_df['original_std'].mean():.2e}\")\n",
    "    \n",
    "    print(\"\\n=== AFTER NORMALIZATION ===\")\n",
    "    print(f\"Mean range: {results_df['normalized_mean'].min():.2e} to {results_df['normalized_mean'].max():.2e}\")\n",
    "    print(f\"Std range: {results_df['normalized_std'].min():.2e} to {results_df['normalized_std'].max():.2e}\")\n",
    "    print(f\"Data range: {results_df['normalized_min'].min():.2e} to {results_df['normalized_max'].max():.2e}\")\n",
    "    print(f\"Average mean: {results_df['normalized_mean'].mean():.2e}\")\n",
    "    print(f\"Average std: {results_df['normalized_std'].mean():.2e}\")\n",
    "    \n",
    "    # Check normalization quality\n",
    "    if NORMALIZATION_TYPE == \"per_file\":\n",
    "        print(\"\\n=== NORMALIZATION QUALITY CHECK ===\")\n",
    "        mean_close_to_zero = np.abs(results_df['normalized_mean']) < 1e-10\n",
    "        std_close_to_one = np.abs(results_df['normalized_std'] - 1.0) < 1e-10\n",
    "        \n",
    "        print(f\"Files with mean ≈ 0: {mean_close_to_zero.sum()}/{len(results_df)} ({mean_close_to_zero.mean()*100:.1f}%)\")\n",
    "        print(f\"Files with std ≈ 1: {std_close_to_one.sum()}/{len(results_df)} ({std_close_to_one.mean()*100:.1f}%)\")\n",
    "        \n",
    "        if mean_close_to_zero.mean() > 0.95 and std_close_to_one.mean() > 0.95:\n",
    "            print(\"✓ Normalization quality: EXCELLENT\")\n",
    "        elif mean_close_to_zero.mean() > 0.8 and std_close_to_one.mean() > 0.8:\n",
    "            print(\"✓ Normalization quality: GOOD\")\n",
    "        else:\n",
    "            print(\"⚠ Normalization quality: NEEDS REVIEW\")\n",
    "    \n",
    "    # Show sample results\n",
    "    print(\"\\n=== SAMPLE RESULTS ===\")\n",
    "    sample_results = results_df.head(3)\n",
    "    for i, (_, row) in enumerate(sample_results.iterrows()):\n",
    "        print(f\"\\nFile {i+1}: {os.path.basename(row['original_file'])}\")\n",
    "        print(f\"  Original: mean={row['original_mean']:.2e}, std={row['original_std']:.2e}\")\n",
    "        print(f\"  Normalized: mean={row['normalized_mean']:.2e}, std={row['normalized_std']:.2e}\")\n",
    "        print(f\"  Output: {os.path.relpath(row['normalized_file'], OUTPUT_DIR)}\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ No normalization results to analyze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TESTING NORMALIZED FILES ===\n",
      "\n",
      "Test 1: sub-01_ses-01_trial_000_Pronounced_Left_A1.npy\n",
      "  ✓ Loaded successfully\n",
      "  Shape: (641,)\n",
      "  Mean: -1.66e-17\n",
      "  Std: 1.00e+00\n",
      "  Range: -3.84e+00 to 2.48e+00\n",
      "  ✓ Normalization verified (mean≈0, std≈1)\n",
      "\n",
      "Test 2: sub-01_ses-01_trial_000_Pronounced_Left_A10.npy\n",
      "  ✓ Loaded successfully\n",
      "  Shape: (641,)\n",
      "  Mean: 1.32e-17\n",
      "  Std: 1.00e+00\n",
      "  Range: -3.06e+00 to 3.07e+00\n",
      "  ✓ Normalization verified (mean≈0, std≈1)\n",
      "\n",
      "Test 3: sub-01_ses-01_trial_000_Pronounced_Left_A11.npy\n",
      "  ✓ Loaded successfully\n",
      "  Shape: (641,)\n",
      "  Mean: 2.88e-17\n",
      "  Std: 1.00e+00\n",
      "  Range: -3.36e+00 to 2.81e+00\n",
      "  ✓ Normalization verified (mean≈0, std≈1)\n"
     ]
    }
   ],
   "source": [
    "# Test loading normalized files\n",
    "if len(normalization_results) > 0:\n",
    "    print(\"=== TESTING NORMALIZED FILES ===\")\n",
    "    \n",
    "    # Test first 3 normalized files\n",
    "    test_files = results_df.head(3)\n",
    "    \n",
    "    for i, (_, row) in enumerate(test_files.iterrows()):\n",
    "        print(f\"\\nTest {i+1}: {os.path.basename(row['normalized_file'])}\")\n",
    "        \n",
    "        try:\n",
    "            # Load normalized data\n",
    "            if row['normalized_file'].endswith('.npy'):\n",
    "                norm_data = np.load(row['normalized_file'])\n",
    "                print(f\"  ✓ Loaded successfully\")\n",
    "                print(f\"  Shape: {norm_data.shape}\")\n",
    "                print(f\"  Mean: {norm_data.mean():.2e}\")\n",
    "                print(f\"  Std: {norm_data.std():.2e}\")\n",
    "                print(f\"  Range: {norm_data.min():.2e} to {norm_data.max():.2e}\")\n",
    "                \n",
    "                # Verify normalization\n",
    "                if NORMALIZATION_TYPE == \"per_file\":\n",
    "                    if abs(norm_data.mean()) < 1e-10 and abs(norm_data.std() - 1.0) < 1e-10:\n",
    "                        print(f\"  ✓ Normalization verified (mean≈0, std≈1)\")\n",
    "                    else:\n",
    "                        print(f\"  ⚠ Normalization may have issues\")\n",
    "                        \n",
    "            else:\n",
    "                print(f\"  ⚠ Unsupported file format for testing\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error loading: {e}\")\n",
    "            \n",
    "else:\n",
    "    print(\"No normalized files to test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FILES SAVED ===\n",
      "✓ 696320 normalized data files\n",
      "✓ Results: D:\\VIT\\IV-Year\\PJT-I\\Speech Imagery Decoding\\Inner_Speech_Dataset\\Dataset\\normalized_data/metadata/normalization_results.csv\n",
      "✓ Results: D:\\VIT\\IV-Year\\PJT-I\\Speech Imagery Decoding\\Inner_Speech_Dataset\\Dataset\\normalized_data/metadata/normalization_results.pkl\n",
      "✓ Summary: D:\\VIT\\IV-Year\\PJT-I\\Speech Imagery Decoding\\Inner_Speech_Dataset\\Dataset\\normalized_data/metadata/normalization_summary.txt\n",
      "\n",
      "=== USAGE ===\n",
      "Load normalized data:\n",
      "  data = np.load('normalized_data/path/to/file.npy')\n",
      "\n",
      "Load results metadata:\n",
      "  results = pd.read_csv('D:\\VIT\\IV-Year\\PJT-I\\Speech Imagery Decoding\\Inner_Speech_Dataset\\Dataset\\normalized_data/metadata/normalization_results.csv')\n",
      "\n",
      "Find specific files:\n",
      "  # Original file path to find normalized version\n",
      "  norm_file = results[results['original_file'] == 'path']['normalized_file'].iloc[0]\n",
      "\n",
      " Z-SCORE NORMALIZATION COMPLETE!\n",
      "Your data is now:\n",
      "  • Each file: mean ≈ 0, std ≈ 1\n",
      "  • Ready for machine learning and analysis\n",
      "  • Saved in: D:\\VIT\\IV-Year\\PJT-I\\Speech Imagery Decoding\\Inner_Speech_Dataset\\Dataset\\normalized_data\n"
     ]
    }
   ],
   "source": [
    "if hasattr(sys.stdout, \"reconfigure\"):\n",
    "    sys.stdout.reconfigure(encoding='utf-8')\n",
    "# Save results and create summary\n",
    "if len(normalization_results) > 0:\n",
    "    # Save normalization results\n",
    "    results_csv = f\"{OUTPUT_DIR}/metadata/normalization_results.csv\"\n",
    "    results_pkl = f\"{OUTPUT_DIR}/metadata/normalization_results.pkl\"\n",
    "    summary_file = f\"{OUTPUT_DIR}/metadata/normalization_summary.txt\"\n",
    "    \n",
    "    results_df.to_csv(results_csv, index=False)\n",
    "    results_df.to_pickle(results_pkl)\n",
    "    \n",
    "    # Create summary file in UTF-8\n",
    "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"Z-SCORE NORMALIZATION SUMMARY\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Input Directory: {INPUT_DIR}\\n\")\n",
    "        f.write(f\"Output Directory: {OUTPUT_DIR}\\n\")\n",
    "        f.write(f\"Normalization Type: {NORMALIZATION_TYPE}\\n\")\n",
    "        f.write(f\"Preserve Structure: {PRESERVE_STRUCTURE}\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Processing Results:\\n\")\n",
    "        f.write(f\"  Files processed: {len(results_df)}\\n\")\n",
    "        f.write(f\"  Success rate: {success_count/(success_count+fail_count)*100:.1f}%\\n\")\n",
    "        f.write(f\"  Primary file type: {primary_type}\\n\\n\")\n",
    "        \n",
    "        if NORMALIZATION_TYPE == \"global\":\n",
    "            f.write(f\"Global Statistics Used:\\n\")\n",
    "            f.write(f\"  Global mean: {global_mean:.6e}\\n\")\n",
    "            f.write(f\"  Global std: {global_std:.6e}\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Before Normalization:\\n\")\n",
    "        f.write(f\"  Mean range: {results_df['original_mean'].min():.2e} to {results_df['original_mean'].max():.2e}\\n\")\n",
    "        f.write(f\"  Std range: {results_df['original_std'].min():.2e} to {results_df['original_std'].max():.2e}\\n\")\n",
    "        f.write(f\"  Data range: {results_df['original_min'].min():.2e} to {results_df['original_max'].max():.2e}\\n\")\n",
    "        f.write(f\"  Average mean: {results_df['original_mean'].mean():.2e}\\n\")\n",
    "        f.write(f\"  Average std: {results_df['original_std'].mean():.2e}\\n\\n\")\n",
    "        \n",
    "        f.write(f\"After Normalization:\\n\")\n",
    "        f.write(f\"  Mean range: {results_df['normalized_mean'].min():.2e} to {results_df['normalized_mean'].max():.2e}\\n\")\n",
    "        f.write(f\"  Std range: {results_df['normalized_std'].min():.2e} to {results_df['normalized_std'].max():.2e}\\n\")\n",
    "        f.write(f\"  Data range: {results_df['normalized_min'].min():.2e} to {results_df['normalized_max'].max():.2e}\\n\")\n",
    "        f.write(f\"  Average mean: {results_df['normalized_mean'].mean():.2e}\\n\")\n",
    "        f.write(f\"  Average std: {results_df['normalized_std'].mean():.2e}\\n\\n\")\n",
    "        \n",
    "        if NORMALIZATION_TYPE == \"per_file\":\n",
    "            mean_close_to_zero = np.abs(results_df['normalized_mean']) < 1e-10\n",
    "            std_close_to_one = np.abs(results_df['normalized_std'] - 1.0) < 1e-10\n",
    "            f.write(f\"Quality Check:\\n\")\n",
    "            f.write(f\"  Files with mean ≈ 0: {mean_close_to_zero.sum()}/{len(results_df)} ({mean_close_to_zero.mean()*100:.1f}%)\\n\")\n",
    "            f.write(f\"  Files with std ≈ 1: {std_close_to_one.sum()}/{len(results_df)} ({std_close_to_one.mean()*100:.1f}%)\\n\")\n",
    "    \n",
    "    # Console output (UTF-8 safe)\n",
    "    print(f\"\\n=== FILES SAVED ===\")\n",
    "    print(f\"✓ {len(results_df)} normalized data files\")\n",
    "    print(f\"✓ Results: {results_csv}\")\n",
    "    print(f\"✓ Results: {results_pkl}\")\n",
    "    print(f\"✓ Summary: {summary_file}\")\n",
    "    \n",
    "    print(f\"\\n=== USAGE ===\")\n",
    "    print(f\"Load normalized data:\")\n",
    "    print(f\"  data = np.load('normalized_data/path/to/file.npy')\")\n",
    "    print(f\"\\nLoad results metadata:\")\n",
    "    print(f\"  results = pd.read_csv('{results_csv}')\")\n",
    "    print(f\"\\nFind specific files:\")\n",
    "    print(f\"  # Original file path to find normalized version\")\n",
    "    print(f\"  norm_file = results[results['original_file'] == 'path']['normalized_file'].iloc[0]\")\n",
    "    \n",
    "    print(f\"\\n Z-SCORE NORMALIZATION COMPLETE!\")\n",
    "    print(f\"Your data is now:\")\n",
    "    if NORMALIZATION_TYPE == \"per_file\":\n",
    "        print(f\"  • Each file: mean ≈ 0, std ≈ 1\")\n",
    "    elif NORMALIZATION_TYPE == \"global\":\n",
    "        print(f\"  • All files normalized using global statistics\")\n",
    "    elif NORMALIZATION_TYPE == \"per_channel\":\n",
    "        print(f\"  • Each channel normalized independently\")\n",
    "    print(f\"  • Ready for machine learning and analysis\")\n",
    "    print(f\"  • Saved in: {OUTPUT_DIR}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No normalization results to save\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
